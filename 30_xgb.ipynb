{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062a9473-ec3c-4f1c-80de-66912972bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from typing import List, Dict, Union, Tuple, NamedTuple\n",
    "from tqdm import tqdm\n",
    "import scml\n",
    "from scml import pandasx as pdx\n",
    "tim = scml.Timer()\n",
    "tim.start()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "percentiles=[.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99]\n",
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "tqdm.pandas()\n",
    "scml.seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafd5a38-0166-47b6-aed4-7ae66bd7bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "job_dir = f\"models/xgb/{ts}\"\n",
    "pathlib.Path(job_dir).mkdir(parents=True, exist_ok=True)\n",
    "num_boost_round: int = 100\n",
    "lr: Tuple[float, float] = (1e-3, 1e-3)\n",
    "feature_fraction: Tuple[float, float] = (1, 1)\n",
    "min_data_in_leaf: Tuple[int, int] = (20, 20)\n",
    "objective: str = \"binary:logistic\"\n",
    "n_trials: int = 1\n",
    "label = \"generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e822fad3-d0a9-4094-9efb-60cde058d454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39120 entries, 0 to 39119\n",
      "Columns: 29787 entries, essay_id to tf_Ä zygomatic\n",
      "dtypes: float32(29772), int16(2), int32(5), int8(1), object(7)\n",
      "memory usage: 4.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"input/features.parquet\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb8e82b-a582-4d2a-88e1-d4a4c85a1fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29775 features\n",
      "['ch_digit_frac', 'ch_len', 'ch_letter_frac', 'ch_punc_frac', 'ch_repeat_char_frac', 'ch_space_frac', 'ch_upper_frac', 'tf_0', 'tf_00', 'tf_000', 'tf_03', 'tf_1', 'tf_10', 'tf_11', 'tf_12', 'tf_13', 'tf_14', 'tf_15', 'tf_16', 'tf_17', 'tf_18', 'tf_19', 'tf_199', 'tf_1990', 'tf_2', 'tf_20', 'tf_200', 'tf_2002', 'tf_21', 'tf_23', 'tf_24', 'tf_25', 'tf_27', 'tf_28', 'tf_3', 'tf_30', 'tf_31', 'tf_32', 'tf_33', 'tf_34', 'tf_38', 'tf_39', 'tf_4', 'tf_40', 'tf_41', 'tf_43', 'tf_45', 'tf_5', 'tf_50', 'tf_538', 'tf_58', 'tf_6', 'tf_60', 'tf_62', 'tf_7', 'tf_70', 'tf_74', 'tf_76', 'tf_79', 'tf_8', 'tf_87', 'tf_9', 'tf_a', 'tf_aa', 'tf_aae', 'tf_aage', 'tf_aaion', 'tf_ab', 'tf_aban', 'tf_abe', 'tf_abel', 'tf_aber', 'tf_abet', 'tf_abeth', 'tf_abil', 'tf_abilites', 'tf_abilitie', 'tf_abilities', 'tf_ability', 'tf_abill', 'tf_abilty', 'tf_abitable', 'tf_abital', 'tf_abl', 'tf_able', 'tf_abled', 'tf_ables', 'tf_abling', 'tf_ablish', 'tf_ablished', 'tf_ablities', 'tf_ablity', 'tf_ably', 'tf_about', 'tf_abra', 'tf_abraeus', 'tf_abs', 'tf_abul', 'tf_aby', 'tf_ac']\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "prefixes = [\"ch_\", \"ws_\", \"ts_\", \"va_\", \"tf_\"]\n",
    "for col in df.columns:\n",
    "    for prefix in prefixes:\n",
    "        if col.startswith(prefix):\n",
    "            features.append(col)\n",
    "features.sort()\n",
    "print(f\"{len(features)} features\\n{features[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41fd2b91-247a-4891-9859-478bb1d42583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val%=0.0293, len(tra)=37,974, len(val)=1,146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generated</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>907</td>\n",
       "      <td>0.791449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>239</td>\n",
       "      <td>0.208551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count   percent\n",
       "generated                 \n",
       "1            907  0.791449\n",
       "0            239  0.208551"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(tra[features], tra[label], test_size=0.2)\n",
    "\n",
    "tra = df[df[\"white_sim\"]>=0.45]\n",
    "val = df[df[\"white_sim\"]<0.45]\n",
    "t = len(tra)\n",
    "v = len(val)\n",
    "n = t+v\n",
    "print(f\"val%={v/n:.4f}, len(tra)={t:,}, len(val)={v:,}\")\n",
    "dtrain = xgb.DMatrix(tra[features], tra[label], enable_categorical=False)\n",
    "dval = xgb.DMatrix(val[features], val[label], enable_categorical=False)\n",
    "pdx.value_counts(val[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df30c12e-601b-4dcb-8ab9-90f3beb13b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.51395\tval-logloss:1.07884\n",
      "[40]\ttrain-logloss:0.12450\tval-logloss:0.36778\n",
      "[80]\ttrain-logloss:0.05951\tval-logloss:0.22819\n",
      "[120]\ttrain-logloss:0.03711\tval-logloss:0.17022\n",
      "[160]\ttrain-logloss:0.02651\tval-logloss:0.13479\n",
      "[200]\ttrain-logloss:0.02038\tval-logloss:0.11529\n",
      "[240]\ttrain-logloss:0.01619\tval-logloss:0.10146\n",
      "[280]\ttrain-logloss:0.01334\tval-logloss:0.09335\n",
      "[320]\ttrain-logloss:0.01129\tval-logloss:0.08700\n",
      "[360]\ttrain-logloss:0.00969\tval-logloss:0.08254\n",
      "[400]\ttrain-logloss:0.00849\tval-logloss:0.07864\n",
      "[440]\ttrain-logloss:0.00752\tval-logloss:0.07500\n",
      "[480]\ttrain-logloss:0.00679\tval-logloss:0.07311\n",
      "[520]\ttrain-logloss:0.00621\tval-logloss:0.07109\n",
      "[560]\ttrain-logloss:0.00575\tval-logloss:0.07022\n",
      "[600]\ttrain-logloss:0.00536\tval-logloss:0.06864\n",
      "[640]\ttrain-logloss:0.00504\tval-logloss:0.06831\n",
      "[680]\ttrain-logloss:0.00476\tval-logloss:0.06800\n",
      "[720]\ttrain-logloss:0.00452\tval-logloss:0.06790\n",
      "[760]\ttrain-logloss:0.00433\tval-logloss:0.06766\n",
      "[800]\ttrain-logloss:0.00415\tval-logloss:0.06722\n",
      "[840]\ttrain-logloss:0.00399\tval-logloss:0.06677\n",
      "[880]\ttrain-logloss:0.00384\tval-logloss:0.06640\n",
      "[920]\ttrain-logloss:0.00372\tval-logloss:0.06595\n",
      "[960]\ttrain-logloss:0.00361\tval-logloss:0.06589\n",
      "[1000]\ttrain-logloss:0.00350\tval-logloss:0.06569\n",
      "[1040]\ttrain-logloss:0.00341\tval-logloss:0.06543\n",
      "[1080]\ttrain-logloss:0.00332\tval-logloss:0.06530\n",
      "[1120]\ttrain-logloss:0.00324\tval-logloss:0.06496\n",
      "[1160]\ttrain-logloss:0.00317\tval-logloss:0.06525\n",
      "[1200]\ttrain-logloss:0.00310\tval-logloss:0.06517\n",
      "[1218]\ttrain-logloss:0.00308\tval-logloss:0.06530\n",
      "best score 0.06493 at iteration 1119\n",
      "CPU times: user 3h 3min 47s, sys: 37min 26s, total: 3h 41min 13s\n",
      "Wall time: 17min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = xgb.train(\n",
    "   params={\n",
    "       \"objective\": objective,\n",
    "       \"learning_rate\": 5e-2,\n",
    "       \"min_child_weight\": 20,\n",
    "       \"colsample_bytree\": 0.5,\n",
    "       \"max_depth\": 6,\n",
    "   },\n",
    "   dtrain=dtrain,\n",
    "   num_boost_round=2000,\n",
    "   evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "   verbose_eval=40,\n",
    "   early_stopping_rounds=100,\n",
    ")\n",
    "print(f\"best score {model.best_score:.5f} at iteration {model.best_iteration}\")\n",
    "model.save_model(f\"{job_dir}/model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6da2370-9acb-42fa-894f-c4da29da6425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.9978\n",
      "y_pred=(1146,)\n",
      "[0.9499928  0.97822237 0.99891245 0.00628967 0.99998033]\n",
      "CPU times: user 312 ms, sys: 400 ms, total: 712 ms\n",
      "Wall time: 59.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_true = val[label].tolist()\n",
    "y_pred = model.predict(data=dval, iteration_range=(0, model.best_iteration+1))\n",
    "auc = roc_auc_score(y_true, y_pred, average=\"macro\")\n",
    "print(f\"auc={auc:.4f}\")\n",
    "print(f\"y_pred={y_pred.shape}\\n{y_pred[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f793a39d-7bce-4472-aef3-bacde89ee929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models/xgb/20240120_041933/importance.csv\n",
      "CPU times: user 21.2 ms, sys: 38.7 ms, total: 59.9 ms\n",
      "Wall time: 5.05 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>importance</th>\n",
       "      <td>535.333862</td>\n",
       "      <td>491.513977</td>\n",
       "      <td>487.654358</td>\n",
       "      <td>362.805176</td>\n",
       "      <td>333.964478</td>\n",
       "      <td>332.914612</td>\n",
       "      <td>226.789703</td>\n",
       "      <td>219.15686</td>\n",
       "      <td>214.638794</td>\n",
       "      <td>211.520065</td>\n",
       "      <td>179.099335</td>\n",
       "      <td>171.318848</td>\n",
       "      <td>149.939621</td>\n",
       "      <td>144.979034</td>\n",
       "      <td>138.28273</td>\n",
       "      <td>124.150146</td>\n",
       "      <td>121.580658</td>\n",
       "      <td>115.805115</td>\n",
       "      <td>114.466164</td>\n",
       "      <td>108.677856</td>\n",
       "      <td>101.883476</td>\n",
       "      <td>101.435951</td>\n",
       "      <td>98.002876</td>\n",
       "      <td>84.509155</td>\n",
       "      <td>76.646042</td>\n",
       "      <td>76.45359</td>\n",
       "      <td>75.889709</td>\n",
       "      <td>69.939545</td>\n",
       "      <td>66.292404</td>\n",
       "      <td>66.05307</td>\n",
       "      <td>65.250473</td>\n",
       "      <td>63.459717</td>\n",
       "      <td>62.241436</td>\n",
       "      <td>56.666737</td>\n",
       "      <td>54.976513</td>\n",
       "      <td>54.656471</td>\n",
       "      <td>53.885494</td>\n",
       "      <td>53.665058</td>\n",
       "      <td>53.12447</td>\n",
       "      <td>52.420853</td>\n",
       "      <td>52.350555</td>\n",
       "      <td>51.53833</td>\n",
       "      <td>50.223473</td>\n",
       "      <td>49.986206</td>\n",
       "      <td>49.040386</td>\n",
       "      <td>47.444031</td>\n",
       "      <td>46.485104</td>\n",
       "      <td>46.267811</td>\n",
       "      <td>40.464008</td>\n",
       "      <td>40.103317</td>\n",
       "      <td>39.966915</td>\n",
       "      <td>38.075565</td>\n",
       "      <td>37.707947</td>\n",
       "      <td>37.261292</td>\n",
       "      <td>36.566113</td>\n",
       "      <td>36.080936</td>\n",
       "      <td>35.468941</td>\n",
       "      <td>33.705811</td>\n",
       "      <td>33.141094</td>\n",
       "      <td>30.172977</td>\n",
       "      <td>29.95191</td>\n",
       "      <td>29.405884</td>\n",
       "      <td>29.22114</td>\n",
       "      <td>28.309143</td>\n",
       "      <td>27.806349</td>\n",
       "      <td>27.620529</td>\n",
       "      <td>26.203878</td>\n",
       "      <td>26.058992</td>\n",
       "      <td>25.645138</td>\n",
       "      <td>25.579153</td>\n",
       "      <td>25.304052</td>\n",
       "      <td>24.991737</td>\n",
       "      <td>24.953922</td>\n",
       "      <td>24.29339</td>\n",
       "      <td>23.767916</td>\n",
       "      <td>22.674658</td>\n",
       "      <td>22.334621</td>\n",
       "      <td>21.925852</td>\n",
       "      <td>21.656654</td>\n",
       "      <td>21.583004</td>\n",
       "      <td>21.053091</td>\n",
       "      <td>20.341541</td>\n",
       "      <td>20.336012</td>\n",
       "      <td>19.925665</td>\n",
       "      <td>19.696354</td>\n",
       "      <td>19.651701</td>\n",
       "      <td>19.599421</td>\n",
       "      <td>19.536987</td>\n",
       "      <td>19.467497</td>\n",
       "      <td>19.375626</td>\n",
       "      <td>18.577335</td>\n",
       "      <td>18.273558</td>\n",
       "      <td>18.098648</td>\n",
       "      <td>18.076061</td>\n",
       "      <td>17.982529</td>\n",
       "      <td>17.886499</td>\n",
       "      <td>17.813538</td>\n",
       "      <td>17.708664</td>\n",
       "      <td>17.696554</td>\n",
       "      <td>17.532536</td>\n",
       "      <td>17.430914</td>\n",
       "      <td>17.187277</td>\n",
       "      <td>17.073383</td>\n",
       "      <td>16.762617</td>\n",
       "      <td>16.444614</td>\n",
       "      <td>16.400848</td>\n",
       "      <td>16.220268</td>\n",
       "      <td>16.089945</td>\n",
       "      <td>16.070419</td>\n",
       "      <td>15.904037</td>\n",
       "      <td>15.884532</td>\n",
       "      <td>15.785007</td>\n",
       "      <td>15.684215</td>\n",
       "      <td>14.927987</td>\n",
       "      <td>14.50132</td>\n",
       "      <td>14.360294</td>\n",
       "      <td>14.077721</td>\n",
       "      <td>14.069798</td>\n",
       "      <td>14.040625</td>\n",
       "      <td>13.981558</td>\n",
       "      <td>13.925736</td>\n",
       "      <td>13.558373</td>\n",
       "      <td>13.070453</td>\n",
       "      <td>13.039894</td>\n",
       "      <td>13.004128</td>\n",
       "      <td>12.855943</td>\n",
       "      <td>12.818004</td>\n",
       "      <td>12.804463</td>\n",
       "      <td>12.793447</td>\n",
       "      <td>12.426529</td>\n",
       "      <td>12.19511</td>\n",
       "      <td>12.099344</td>\n",
       "      <td>12.074789</td>\n",
       "      <td>12.037127</td>\n",
       "      <td>11.92334</td>\n",
       "      <td>11.868005</td>\n",
       "      <td>11.733463</td>\n",
       "      <td>11.649146</td>\n",
       "      <td>11.290364</td>\n",
       "      <td>11.227482</td>\n",
       "      <td>10.756605</td>\n",
       "      <td>10.752293</td>\n",
       "      <td>10.73936</td>\n",
       "      <td>10.539305</td>\n",
       "      <td>10.441528</td>\n",
       "      <td>10.283181</td>\n",
       "      <td>10.095117</td>\n",
       "      <td>9.960876</td>\n",
       "      <td>9.8633</td>\n",
       "      <td>9.83662</td>\n",
       "      <td>9.826649</td>\n",
       "      <td>9.656018</td>\n",
       "      <td>9.445007</td>\n",
       "      <td>9.382985</td>\n",
       "      <td>9.347935</td>\n",
       "      <td>9.164958</td>\n",
       "      <td>9.125072</td>\n",
       "      <td>8.971872</td>\n",
       "      <td>8.706679</td>\n",
       "      <td>8.687664</td>\n",
       "      <td>8.57763</td>\n",
       "      <td>8.3493</td>\n",
       "      <td>8.201652</td>\n",
       "      <td>7.949665</td>\n",
       "      <td>7.875024</td>\n",
       "      <td>7.80212</td>\n",
       "      <td>7.797144</td>\n",
       "      <td>7.544501</td>\n",
       "      <td>7.445221</td>\n",
       "      <td>7.244454</td>\n",
       "      <td>7.150042</td>\n",
       "      <td>6.944552</td>\n",
       "      <td>6.941762</td>\n",
       "      <td>6.930393</td>\n",
       "      <td>6.874111</td>\n",
       "      <td>6.838174</td>\n",
       "      <td>6.832476</td>\n",
       "      <td>6.773543</td>\n",
       "      <td>6.651915</td>\n",
       "      <td>6.580213</td>\n",
       "      <td>6.485192</td>\n",
       "      <td>6.468338</td>\n",
       "      <td>6.456057</td>\n",
       "      <td>6.327619</td>\n",
       "      <td>6.102661</td>\n",
       "      <td>6.045862</td>\n",
       "      <td>6.010983</td>\n",
       "      <td>5.981054</td>\n",
       "      <td>5.956406</td>\n",
       "      <td>5.908396</td>\n",
       "      <td>5.868488</td>\n",
       "      <td>5.848146</td>\n",
       "      <td>5.813245</td>\n",
       "      <td>5.774518</td>\n",
       "      <td>5.641491</td>\n",
       "      <td>5.617348</td>\n",
       "      <td>5.562448</td>\n",
       "      <td>5.496105</td>\n",
       "      <td>5.446288</td>\n",
       "      <td>4.977261</td>\n",
       "      <td>4.940655</td>\n",
       "      <td>4.803391</td>\n",
       "      <td>4.802198</td>\n",
       "      <td>4.739406</td>\n",
       "      <td>4.701181</td>\n",
       "      <td>4.675903</td>\n",
       "      <td>4.542944</td>\n",
       "      <td>4.366622</td>\n",
       "      <td>4.336646</td>\n",
       "      <td>4.335607</td>\n",
       "      <td>4.310486</td>\n",
       "      <td>4.16504</td>\n",
       "      <td>4.124278</td>\n",
       "      <td>4.074976</td>\n",
       "      <td>4.058128</td>\n",
       "      <td>4.047112</td>\n",
       "      <td>4.033809</td>\n",
       "      <td>3.958862</td>\n",
       "      <td>3.883852</td>\n",
       "      <td>3.794348</td>\n",
       "      <td>3.684112</td>\n",
       "      <td>3.624991</td>\n",
       "      <td>3.612345</td>\n",
       "      <td>3.540681</td>\n",
       "      <td>3.475764</td>\n",
       "      <td>3.435429</td>\n",
       "      <td>3.407918</td>\n",
       "      <td>3.379809</td>\n",
       "      <td>3.342614</td>\n",
       "      <td>3.306682</td>\n",
       "      <td>3.263934</td>\n",
       "      <td>3.259813</td>\n",
       "      <td>3.166806</td>\n",
       "      <td>3.154149</td>\n",
       "      <td>3.078102</td>\n",
       "      <td>2.938603</td>\n",
       "      <td>2.877229</td>\n",
       "      <td>2.867808</td>\n",
       "      <td>2.849793</td>\n",
       "      <td>2.829164</td>\n",
       "      <td>2.787444</td>\n",
       "      <td>2.77756</td>\n",
       "      <td>2.739072</td>\n",
       "      <td>2.726932</td>\n",
       "      <td>2.602082</td>\n",
       "      <td>2.598131</td>\n",
       "      <td>2.557087</td>\n",
       "      <td>2.551506</td>\n",
       "      <td>2.533246</td>\n",
       "      <td>2.490699</td>\n",
       "      <td>2.484266</td>\n",
       "      <td>2.471044</td>\n",
       "      <td>2.448205</td>\n",
       "      <td>2.415814</td>\n",
       "      <td>2.31446</td>\n",
       "      <td>2.101922</td>\n",
       "      <td>2.099108</td>\n",
       "      <td>2.086792</td>\n",
       "      <td>2.078264</td>\n",
       "      <td>1.99505</td>\n",
       "      <td>1.982076</td>\n",
       "      <td>1.962839</td>\n",
       "      <td>1.936546</td>\n",
       "      <td>1.923023</td>\n",
       "      <td>1.905253</td>\n",
       "      <td>1.903507</td>\n",
       "      <td>1.870733</td>\n",
       "      <td>1.86094</td>\n",
       "      <td>1.827483</td>\n",
       "      <td>1.821375</td>\n",
       "      <td>1.767868</td>\n",
       "      <td>1.741594</td>\n",
       "      <td>1.710009</td>\n",
       "      <td>1.684256</td>\n",
       "      <td>1.683862</td>\n",
       "      <td>1.640992</td>\n",
       "      <td>1.616394</td>\n",
       "      <td>1.573892</td>\n",
       "      <td>1.500577</td>\n",
       "      <td>1.498889</td>\n",
       "      <td>1.496079</td>\n",
       "      <td>1.44943</td>\n",
       "      <td>1.437862</td>\n",
       "      <td>1.391529</td>\n",
       "      <td>1.379298</td>\n",
       "      <td>1.369891</td>\n",
       "      <td>1.338372</td>\n",
       "      <td>1.248961</td>\n",
       "      <td>1.240137</td>\n",
       "      <td>1.197424</td>\n",
       "      <td>1.115942</td>\n",
       "      <td>1.110137</td>\n",
       "      <td>1.07143</td>\n",
       "      <td>1.045055</td>\n",
       "      <td>1.04131</td>\n",
       "      <td>1.03263</td>\n",
       "      <td>1.030561</td>\n",
       "      <td>1.013166</td>\n",
       "      <td>0.980068</td>\n",
       "      <td>0.973082</td>\n",
       "      <td>0.916443</td>\n",
       "      <td>0.817195</td>\n",
       "      <td>0.816088</td>\n",
       "      <td>0.770794</td>\n",
       "      <td>0.759886</td>\n",
       "      <td>0.702525</td>\n",
       "      <td>0.693356</td>\n",
       "      <td>0.686387</td>\n",
       "      <td>0.643064</td>\n",
       "      <td>0.625346</td>\n",
       "      <td>0.590192</td>\n",
       "      <td>0.586884</td>\n",
       "      <td>0.576589</td>\n",
       "      <td>0.562961</td>\n",
       "      <td>0.556713</td>\n",
       "      <td>0.546316</td>\n",
       "      <td>0.301447</td>\n",
       "      <td>0.283508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <td>ts_polysyllable_frac</td>\n",
       "      <td>ts_syllables_per_word</td>\n",
       "      <td>tf_Ä hey</td>\n",
       "      <td>tf_Ä performance</td>\n",
       "      <td>tf_Ä super</td>\n",
       "      <td>ch_space_frac</td>\n",
       "      <td>tf_Ä essay</td>\n",
       "      <td>tf_Ä ensures</td>\n",
       "      <td>ts_flesch_kincaid_grade</td>\n",
       "      <td>tf_Ä goals</td>\n",
       "      <td>tf_Ä essential</td>\n",
       "      <td>tf_Ä totally</td>\n",
       "      <td>tf_Ä because</td>\n",
       "      <td>tf_th</td>\n",
       "      <td>tf_Ä additionally</td>\n",
       "      <td>tf_Ä ultimately</td>\n",
       "      <td>tf_Ä would</td>\n",
       "      <td>tf_Ä importance</td>\n",
       "      <td>tf_Ä grader</td>\n",
       "      <td>tf_Ä stuff</td>\n",
       "      <td>tf_Ä firstly</td>\n",
       "      <td>tf_Ä nt</td>\n",
       "      <td>tf_Ä very</td>\n",
       "      <td>tf_Ä attempt</td>\n",
       "      <td>ch_punc_frac</td>\n",
       "      <td>ts_coleman_liau_index</td>\n",
       "      <td>tf_Ä sustainable</td>\n",
       "      <td>tf_Ä success</td>\n",
       "      <td>tf_Ä conclusion</td>\n",
       "      <td>ch_letter_frac</td>\n",
       "      <td>tf_Ä china</td>\n",
       "      <td>tf_Ä president</td>\n",
       "      <td>tf_Ä and</td>\n",
       "      <td>tf_Ä facial</td>\n",
       "      <td>tf_Ä venus</td>\n",
       "      <td>tf_Ä thank</td>\n",
       "      <td>ws_sent_len_std</td>\n",
       "      <td>tf_Ä important</td>\n",
       "      <td>ts_smog_index</td>\n",
       "      <td>tf_Ä like</td>\n",
       "      <td>tf_Ä address</td>\n",
       "      <td>tf_Ä europe</td>\n",
       "      <td>tf_Ä animals</td>\n",
       "      <td>tf_Ä perspectives</td>\n",
       "      <td>tf_Ä computer</td>\n",
       "      <td>tf_Ä driverless</td>\n",
       "      <td>ws_sent_len_delta_mean</td>\n",
       "      <td>tf_Ä 8</td>\n",
       "      <td>ts_lexicon_count</td>\n",
       "      <td>tf_Ä teacher</td>\n",
       "      <td>tf_Ä potential</td>\n",
       "      <td>tf_Ä extracurricular</td>\n",
       "      <td>tf_Ä dear</td>\n",
       "      <td>ch_digit_frac</td>\n",
       "      <td>tf_Ä informed</td>\n",
       "      <td>tf_Ä significant</td>\n",
       "      <td>tf_Ä seagoing</td>\n",
       "      <td>tf_Ä achieve</td>\n",
       "      <td>tf_Ä car</td>\n",
       "      <td>ws_sent_len_delta_std</td>\n",
       "      <td>tf_Ä although</td>\n",
       "      <td>tf_Ä nasa</td>\n",
       "      <td>tf_Ä electors</td>\n",
       "      <td>tf_Ä phone</td>\n",
       "      <td>tf_Ä then</td>\n",
       "      <td>tf_Ä advantages</td>\n",
       "      <td>tf_Ä smaller</td>\n",
       "      <td>tf_Ä writing</td>\n",
       "      <td>tf_Ä activity</td>\n",
       "      <td>tf_Ä humans</td>\n",
       "      <td>tf_Ä secondly</td>\n",
       "      <td>tf_Ä cool</td>\n",
       "      <td>ts_monosyllable_frac</td>\n",
       "      <td>tf_Ä plus</td>\n",
       "      <td>tf_Ä skills</td>\n",
       "      <td>tf_Ä human</td>\n",
       "      <td>tf_Ä us</td>\n",
       "      <td>tf_Ä provide</td>\n",
       "      <td>tf_Ä overall</td>\n",
       "      <td>tf_Ä ensure</td>\n",
       "      <td>tf_Ä experiences</td>\n",
       "      <td>ts_dale_chall_readability_score</td>\n",
       "      <td>tf_Ä almost</td>\n",
       "      <td>tf_Ä paragraph</td>\n",
       "      <td>tf_Ä probably</td>\n",
       "      <td>tf_Ä do</td>\n",
       "      <td>tf_Ä driving</td>\n",
       "      <td>tf_Ä career</td>\n",
       "      <td>ts_automated_readability_index</td>\n",
       "      <td>tf_Ä reducing</td>\n",
       "      <td>tf_Ä cars</td>\n",
       "      <td>tf_Ä unique</td>\n",
       "      <td>tf_Ä hand</td>\n",
       "      <td>tf_Ä phones</td>\n",
       "      <td>tf_Ä lead</td>\n",
       "      <td>tf_Ä consider</td>\n",
       "      <td>tf_Ä allows</td>\n",
       "      <td>ch_len</td>\n",
       "      <td>tf_Ä resources</td>\n",
       "      <td>tf_Ä least</td>\n",
       "      <td>tf_Ä percent</td>\n",
       "      <td>tf_Ä you</td>\n",
       "      <td>tf_Ä everyday</td>\n",
       "      <td>tf_Ä argue</td>\n",
       "      <td>tf_Ä text</td>\n",
       "      <td>tf_Ä reduce</td>\n",
       "      <td>tf_Ä service</td>\n",
       "      <td>ts_sentence_count</td>\n",
       "      <td>tf_Ä smog</td>\n",
       "      <td>tf_Ä let</td>\n",
       "      <td>tf_Ä the</td>\n",
       "      <td>tf_Ä furthermore</td>\n",
       "      <td>tf_Ä fair</td>\n",
       "      <td>tf_Ä go</td>\n",
       "      <td>tf_Ä grade</td>\n",
       "      <td>tf_Ä day</td>\n",
       "      <td>tf_Ä many</td>\n",
       "      <td>tf_Ä principal</td>\n",
       "      <td>ts_difficult_words</td>\n",
       "      <td>tf_Ä community</td>\n",
       "      <td>tf_Ä difficult</td>\n",
       "      <td>tf_Ä sense</td>\n",
       "      <td>tf_Ä states</td>\n",
       "      <td>ts_gunning_fog</td>\n",
       "      <td>tf_Ä explore</td>\n",
       "      <td>tf_Ä agree</td>\n",
       "      <td>va_valence_mean</td>\n",
       "      <td>tf_Ä might</td>\n",
       "      <td>tf_Ä so</td>\n",
       "      <td>tf_Ä though</td>\n",
       "      <td>tf_Ä will</td>\n",
       "      <td>tf_Ä earth</td>\n",
       "      <td>tf_Ä most</td>\n",
       "      <td>tf_Ä finally</td>\n",
       "      <td>tf_Ä computers</td>\n",
       "      <td>tf_Ä sincerely</td>\n",
       "      <td>tf_Ä experience</td>\n",
       "      <td>tf_Ä true</td>\n",
       "      <td>tf_Ä students</td>\n",
       "      <td>tf_Ä really</td>\n",
       "      <td>tf_Ä health</td>\n",
       "      <td>tf_Ä i</td>\n",
       "      <td>tf_Ä school</td>\n",
       "      <td>tf_Ä learn</td>\n",
       "      <td>tf_Ä public</td>\n",
       "      <td>tf_Ä technology</td>\n",
       "      <td>tf_Ä support</td>\n",
       "      <td>tf_Ä due</td>\n",
       "      <td>tf_Ä my</td>\n",
       "      <td>tf_Ä transportation</td>\n",
       "      <td>ts_spache_readability</td>\n",
       "      <td>tf_Ä if</td>\n",
       "      <td>tf_Ä required</td>\n",
       "      <td>tf_Ä student</td>\n",
       "      <td>tf_Ä could</td>\n",
       "      <td>ch_upper_frac</td>\n",
       "      <td>tf_Ä impact</td>\n",
       "      <td>tf_Ä name</td>\n",
       "      <td>tf_Ä after</td>\n",
       "      <td>va_arousal_mean</td>\n",
       "      <td>tf_Ä benefits</td>\n",
       "      <td>tf_Ä both</td>\n",
       "      <td>ts_words_per_sent</td>\n",
       "      <td>tf_Ä what</td>\n",
       "      <td>tf_Ä should</td>\n",
       "      <td>tf_Ä much</td>\n",
       "      <td>tf_Ä reasons</td>\n",
       "      <td>tf_Ä means</td>\n",
       "      <td>tf_Ä limiting</td>\n",
       "      <td>ts_syllable_count</td>\n",
       "      <td>tf_Ä not</td>\n",
       "      <td>tf_Ä our</td>\n",
       "      <td>ts_syllables_per_sent</td>\n",
       "      <td>tf_Ä your</td>\n",
       "      <td>ts_flesch_reading_ease</td>\n",
       "      <td>tf_Ä am</td>\n",
       "      <td>tf_Ä point</td>\n",
       "      <td>tf_Ä it</td>\n",
       "      <td>tf_Ä they</td>\n",
       "      <td>tf_Ä know</td>\n",
       "      <td>tf_Ä at</td>\n",
       "      <td>tf_Ä get</td>\n",
       "      <td>va_dominance_mean</td>\n",
       "      <td>tf_Ä focus</td>\n",
       "      <td>tf_Ä own</td>\n",
       "      <td>ws_sent_len_mean</td>\n",
       "      <td>tf_Ä everything</td>\n",
       "      <td>tf_Ä kids</td>\n",
       "      <td>tf_Ä we</td>\n",
       "      <td>tf_Ä hard</td>\n",
       "      <td>tf_Ä over</td>\n",
       "      <td>tf_Ä home</td>\n",
       "      <td>tf_Ä think</td>\n",
       "      <td>tf_Ä may</td>\n",
       "      <td>tf_Ä can</td>\n",
       "      <td>tf_Ä studying</td>\n",
       "      <td>tf_Ä thing</td>\n",
       "      <td>tf_Ä two</td>\n",
       "      <td>tf_Ä often</td>\n",
       "      <td>tf_Ä example</td>\n",
       "      <td>tf_Ä said</td>\n",
       "      <td>tf_Ä all</td>\n",
       "      <td>tf_Ä while</td>\n",
       "      <td>tf_Ä first</td>\n",
       "      <td>ts_mcalpine_eflaw</td>\n",
       "      <td>tf_Ä sports</td>\n",
       "      <td>tf_Ä helping</td>\n",
       "      <td>tf_Ä was</td>\n",
       "      <td>tf_Ä done</td>\n",
       "      <td>tf_Ä last</td>\n",
       "      <td>tf_Ä feedback</td>\n",
       "      <td>tf_Ä believe</td>\n",
       "      <td>tf_Ä just</td>\n",
       "      <td>tf_Ä an</td>\n",
       "      <td>tf_Ä people</td>\n",
       "      <td>tf_Ä another</td>\n",
       "      <td>tf_Ä why</td>\n",
       "      <td>tf_Ä who</td>\n",
       "      <td>tf_Ä reason</td>\n",
       "      <td>tf_Ä learning</td>\n",
       "      <td>tf_Ä schools</td>\n",
       "      <td>tf_Ä one</td>\n",
       "      <td>tf_Ä no</td>\n",
       "      <td>tf_Ä keep</td>\n",
       "      <td>tf_Ä but</td>\n",
       "      <td>tf_Ä going</td>\n",
       "      <td>tf_Ä about</td>\n",
       "      <td>tf_Ä were</td>\n",
       "      <td>tf_Ä than</td>\n",
       "      <td>tf_Ä stay</td>\n",
       "      <td>tf_Ä put</td>\n",
       "      <td>tf_Ä good</td>\n",
       "      <td>va_valence_std</td>\n",
       "      <td>tf_Ä be</td>\n",
       "      <td>tf_Ä make</td>\n",
       "      <td>tf_Ä always</td>\n",
       "      <td>tf_Ä now</td>\n",
       "      <td>tf_Ä how</td>\n",
       "      <td>tf_Ä out</td>\n",
       "      <td>tf_Ä is</td>\n",
       "      <td>tf_Ä this</td>\n",
       "      <td>tf_Ä with</td>\n",
       "      <td>tf_Ä to</td>\n",
       "      <td>tf_Ä its</td>\n",
       "      <td>tf_Ä being</td>\n",
       "      <td>ts_linsear_write_formula</td>\n",
       "      <td>tf_Ä create</td>\n",
       "      <td>tf_Ä say</td>\n",
       "      <td>tf_Ä sure</td>\n",
       "      <td>tf_Ä person</td>\n",
       "      <td>tf_Ä using</td>\n",
       "      <td>tf_Ä down</td>\n",
       "      <td>tf_Ä which</td>\n",
       "      <td>ch_repeat_char_frac</td>\n",
       "      <td>tf_Ä for</td>\n",
       "      <td>tf_Ä same</td>\n",
       "      <td>tf_Ä take</td>\n",
       "      <td>tf_Ä are</td>\n",
       "      <td>tf_Ä more</td>\n",
       "      <td>tf_Ä in</td>\n",
       "      <td>tf_Ä when</td>\n",
       "      <td>tf_Ä there</td>\n",
       "      <td>tf_Ä cause</td>\n",
       "      <td>tf_Ä others</td>\n",
       "      <td>tf_Ä fun</td>\n",
       "      <td>tf_Ä benefit</td>\n",
       "      <td>tf_Ä things</td>\n",
       "      <td>tf_Ä every</td>\n",
       "      <td>tf_Ä doing</td>\n",
       "      <td>va_dominance_std</td>\n",
       "      <td>tf_Ä their</td>\n",
       "      <td>tf_Ä life</td>\n",
       "      <td>tf_Ä lives</td>\n",
       "      <td>tf_Ä s</td>\n",
       "      <td>tf_Ä during</td>\n",
       "      <td>tf_Ä up</td>\n",
       "      <td>tf_Ä have</td>\n",
       "      <td>tf_Ä part</td>\n",
       "      <td>tf_Ä a</td>\n",
       "      <td>tf_Ä time</td>\n",
       "      <td>tf_Ä only</td>\n",
       "      <td>tf_Ä also</td>\n",
       "      <td>tf_Ä that</td>\n",
       "      <td>va_arousal_std</td>\n",
       "      <td>tf_Ä different</td>\n",
       "      <td>tf_Ä around</td>\n",
       "      <td>tf_Ä or</td>\n",
       "      <td>tf_Ä them</td>\n",
       "      <td>tf_Ä on</td>\n",
       "      <td>tf_Ä instead</td>\n",
       "      <td>tf_Ä some</td>\n",
       "      <td>tf_Ä as</td>\n",
       "      <td>tf_Ä new</td>\n",
       "      <td>tf_Ä by</td>\n",
       "      <td>tf_Ä of</td>\n",
       "      <td>tf_Ä want</td>\n",
       "      <td>tf_Ä these</td>\n",
       "      <td>tf_Ä even</td>\n",
       "      <td>tf_Ä way</td>\n",
       "      <td>tf_Ä better</td>\n",
       "      <td>tf_Ä number</td>\n",
       "      <td>tf_Ä able</td>\n",
       "      <td>tf_Ä other</td>\n",
       "      <td>tf_Ä see</td>\n",
       "      <td>tf_Ä work</td>\n",
       "      <td>tf_Ä idea</td>\n",
       "      <td>tf_Ä from</td>\n",
       "      <td>tf_Ä such</td>\n",
       "      <td>tf_Ä any</td>\n",
       "      <td>tf_Ä lot</td>\n",
       "      <td>tf_Ä feel</td>\n",
       "      <td>tf_Ä look</td>\n",
       "      <td>tf_Ä future</td>\n",
       "      <td>tf_Ä help</td>\n",
       "      <td>tf_Ä well</td>\n",
       "      <td>tf_Ä someone</td>\n",
       "      <td>tf_Ä great</td>\n",
       "      <td>tf_Ä takes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0                      1           2    \\\n",
       "importance            535.333862             491.513977  487.654358   \n",
       "feature     ts_polysyllable_frac  ts_syllables_per_word     tf_Ä hey   \n",
       "\n",
       "                        3           4              5           6    \\\n",
       "importance       362.805176  333.964478     332.914612  226.789703   \n",
       "feature     tf_Ä performance   tf_Ä super  ch_space_frac   tf_Ä essay   \n",
       "\n",
       "                    7                        8           9              10   \\\n",
       "importance    219.15686               214.638794  211.520065     179.099335   \n",
       "feature     tf_Ä ensures  ts_flesch_kincaid_grade   tf_Ä goals  tf_Ä essential   \n",
       "\n",
       "                    11           12          13                14   \\\n",
       "importance   171.318848   149.939621  144.979034         138.28273   \n",
       "feature     tf_Ä totally  tf_Ä because       tf_th  tf_Ä additionally   \n",
       "\n",
       "                       15          16              17          18   \\\n",
       "importance      124.150146  121.580658      115.805115  114.466164   \n",
       "feature     tf_Ä ultimately   tf_Ä would  tf_Ä importance  tf_Ä grader   \n",
       "\n",
       "                   19           20          21         22           23   \\\n",
       "importance  108.677856   101.883476  101.435951  98.002876    84.509155   \n",
       "feature      tf_Ä stuff  tf_Ä firstly      tf_Ä nt   tf_Ä very  tf_Ä attempt   \n",
       "\n",
       "                     24                     25               26           27   \\\n",
       "importance     76.646042               76.45359        75.889709    69.939545   \n",
       "feature     ch_punc_frac  ts_coleman_liau_index  tf_Ä sustainable  tf_Ä success   \n",
       "\n",
       "                       28              29         30             31   \\\n",
       "importance       66.292404        66.05307  65.250473      63.459717   \n",
       "feature     tf_Ä conclusion  ch_letter_frac  tf_Ä china  tf_Ä president   \n",
       "\n",
       "                  32          33         34         35               36   \\\n",
       "importance  62.241436   56.666737  54.976513  54.656471        53.885494   \n",
       "feature       tf_Ä and  tf_Ä facial  tf_Ä venus  tf_Ä thank  ws_sent_len_std   \n",
       "\n",
       "                      37             38         39           40          41   \\\n",
       "importance      53.665058       53.12447  52.420853    52.350555    51.53833   \n",
       "feature     tf_Ä important  ts_smog_index   tf_Ä like  tf_Ä address  tf_Ä europe   \n",
       "\n",
       "                    42                43            44              45   \\\n",
       "importance    50.223473         49.986206     49.040386       47.444031   \n",
       "feature     tf_Ä animals  tf_Ä perspectives  tf_Ä computer  tf_Ä driverless   \n",
       "\n",
       "                               46         47                48           49   \\\n",
       "importance               46.485104  46.267811         40.464008    40.103317   \n",
       "feature     ws_sent_len_delta_mean      tf_Ä 8  ts_lexicon_count  tf_Ä teacher   \n",
       "\n",
       "                      50                   51         52             53   \\\n",
       "importance      39.966915            38.075565  37.707947      37.261292   \n",
       "feature     tf_Ä potential  tf_Ä extracurricular   tf_Ä dear  ch_digit_frac   \n",
       "\n",
       "                     54               55            56           57   \\\n",
       "importance     36.566113        36.080936     35.468941    33.705811   \n",
       "feature     tf_Ä informed  tf_Ä significant  tf_Ä seagoing  tf_Ä achieve   \n",
       "\n",
       "                  58                     59            60         61   \\\n",
       "importance  33.141094              30.172977      29.95191  29.405884   \n",
       "feature       tf_Ä car  ws_sent_len_delta_std  tf_Ä although   tf_Ä nasa   \n",
       "\n",
       "                     62         63         64              65           66   \\\n",
       "importance      29.22114  28.309143  27.806349       27.620529    26.203878   \n",
       "feature     tf_Ä electors  tf_Ä phone   tf_Ä then  tf_Ä advantages  tf_Ä smaller   \n",
       "\n",
       "                    67            68          69            70         71   \\\n",
       "importance    26.058992     25.645138   25.579153     25.304052  24.991737   \n",
       "feature     tf_Ä writing  tf_Ä activity  tf_Ä humans  tf_Ä secondly   tf_Ä cool   \n",
       "\n",
       "                             72        73          74         75         76   \\\n",
       "importance             24.953922  24.29339   23.767916  22.674658  22.334621   \n",
       "feature     ts_monosyllable_frac  tf_Ä plus  tf_Ä skills  tf_Ä human     tf_Ä us   \n",
       "\n",
       "                    77           78          79               80   \\\n",
       "importance    21.925852    21.656654   21.583004        21.053091   \n",
       "feature     tf_Ä provide  tf_Ä overall  tf_Ä ensure  tf_Ä experiences   \n",
       "\n",
       "                                        81          82             83   \\\n",
       "importance                        20.341541   20.336012      19.925665   \n",
       "feature     ts_dale_chall_readability_score  tf_Ä almost  tf_Ä paragraph   \n",
       "\n",
       "                     84         85           86          87   \\\n",
       "importance     19.696354  19.651701    19.599421   19.536987   \n",
       "feature     tf_Ä probably     tf_Ä do  tf_Ä driving  tf_Ä career   \n",
       "\n",
       "                                       88            89         90   \\\n",
       "importance                       19.467497     19.375626  18.577335   \n",
       "feature     ts_automated_readability_index  tf_Ä reducing   tf_Ä cars   \n",
       "\n",
       "                   91         92          93         94            95   \\\n",
       "importance   18.273558  18.098648   18.076061  17.982529     17.886499   \n",
       "feature     tf_Ä unique   tf_Ä hand  tf_Ä phones   tf_Ä lead  tf_Ä consider   \n",
       "\n",
       "                   96         97             98         99           100  \\\n",
       "importance   17.813538  17.708664      17.696554  17.532536    17.430914   \n",
       "feature     tf_Ä allows     ch_len  tf_Ä resources  tf_Ä least  tf_Ä percent   \n",
       "\n",
       "                  101           102        103        104         105  \\\n",
       "importance  17.187277     17.073383  16.762617  16.444614   16.400848   \n",
       "feature       tf_Ä you  tf_Ä everyday  tf_Ä argue   tf_Ä text  tf_Ä reduce   \n",
       "\n",
       "                    106                107        108        109        110  \\\n",
       "importance    16.220268          16.089945  16.070419  15.904037  15.884532   \n",
       "feature     tf_Ä service  ts_sentence_count   tf_Ä smog    tf_Ä let    tf_Ä the   \n",
       "\n",
       "                        111        112        113        114        115  \\\n",
       "importance        15.785007  15.684215  14.927987   14.50132  14.360294   \n",
       "feature     tf_Ä furthermore   tf_Ä fair     tf_Ä go  tf_Ä grade    tf_Ä day   \n",
       "\n",
       "                  116            117                 118            119  \\\n",
       "importance  14.077721      14.069798           14.040625      13.981558   \n",
       "feature      tf_Ä many  tf_Ä principal  ts_difficult_words  tf_Ä community   \n",
       "\n",
       "                      120        121         122             123          124  \\\n",
       "importance      13.925736  13.558373   13.070453       13.039894    13.004128   \n",
       "feature     tf_Ä difficult  tf_Ä sense  tf_Ä states  ts_gunning_fog  tf_Ä explore   \n",
       "\n",
       "                  125              126        127        128         129  \\\n",
       "importance  12.855943        12.818004  12.804463  12.793447   12.426529   \n",
       "feature     tf_Ä agree  va_valence_mean  tf_Ä might     tf_Ä so  tf_Ä though   \n",
       "\n",
       "                 130        131        132          133            134  \\\n",
       "importance  12.19511  12.099344  12.074789    12.037127       11.92334   \n",
       "feature     tf_Ä will  tf_Ä earth   tf_Ä most  tf_Ä finally  tf_Ä computers   \n",
       "\n",
       "                      135             136        137           138  \\\n",
       "importance      11.868005       11.733463  11.649146     11.290364   \n",
       "feature     tf_Ä sincerely  tf_Ä experience   tf_Ä true  tf_Ä students   \n",
       "\n",
       "                   139         140        141         142        143  \\\n",
       "importance   11.227482   10.756605  10.752293    10.73936  10.539305   \n",
       "feature     tf_Ä really  tf_Ä health      tf_Ä i  tf_Ä school  tf_Ä learn   \n",
       "\n",
       "                   144             145          146       147     148  \\\n",
       "importance   10.441528       10.283181    10.095117  9.960876  9.8633   \n",
       "feature     tf_Ä public  tf_Ä technology  tf_Ä support   tf_Ä due  tf_Ä my   \n",
       "\n",
       "                           149                    150       151           152  \\\n",
       "importance             9.83662               9.826649  9.656018      9.445007   \n",
       "feature     tf_Ä transportation  ts_spache_readability    tf_Ä if  tf_Ä required   \n",
       "\n",
       "                    153        154            155         156       157  \\\n",
       "importance     9.382985   9.347935       9.164958    9.125072  8.971872   \n",
       "feature     tf_Ä student  tf_Ä could  ch_upper_frac  tf_Ä impact  tf_Ä name   \n",
       "\n",
       "                  158              159           160       161  \\\n",
       "importance   8.706679         8.687664       8.57763    8.3493   \n",
       "feature     tf_Ä after  va_arousal_mean  tf_Ä benefits  tf_Ä both   \n",
       "\n",
       "                          162       163         164       165          166  \\\n",
       "importance           8.201652  7.949665    7.875024   7.80212     7.797144   \n",
       "feature     ts_words_per_sent  tf_Ä what  tf_Ä should  tf_Ä much  tf_Ä reasons   \n",
       "\n",
       "                  167           168                169       170       171  \\\n",
       "importance   7.544501      7.445221           7.244454  7.150042  6.944552   \n",
       "feature     tf_Ä means  tf_Ä limiting  ts_syllable_count   tf_Ä not   tf_Ä our   \n",
       "\n",
       "                              172       173                     174       175  \\\n",
       "importance               6.941762  6.930393                6.874111  6.838174   \n",
       "feature     ts_syllables_per_sent  tf_Ä your  ts_flesch_reading_ease    tf_Ä am   \n",
       "\n",
       "                  176       177       178       179       180       181  \\\n",
       "importance   6.832476  6.773543  6.651915  6.580213  6.485192  6.468338   \n",
       "feature     tf_Ä point    tf_Ä it  tf_Ä they  tf_Ä know    tf_Ä at   tf_Ä get   \n",
       "\n",
       "                          182        183       184               185  \\\n",
       "importance           6.456057   6.327619  6.102661          6.045862   \n",
       "feature     va_dominance_mean  tf_Ä focus   tf_Ä own  ws_sent_len_mean   \n",
       "\n",
       "                       186       187       188       189       190       191  \\\n",
       "importance        6.010983  5.981054  5.956406  5.908396  5.868488  5.848146   \n",
       "feature     tf_Ä everything  tf_Ä kids    tf_Ä we  tf_Ä hard  tf_Ä over  tf_Ä home   \n",
       "\n",
       "                  192       193       194           195        196       197  \\\n",
       "importance   5.813245  5.774518  5.641491      5.617348   5.562448  5.496105   \n",
       "feature     tf_Ä think   tf_Ä may   tf_Ä can  tf_Ä studying  tf_Ä thing   tf_Ä two   \n",
       "\n",
       "                  198          199       200       201        202        203  \\\n",
       "importance   5.446288     4.977261  4.940655  4.803391   4.802198   4.739406   \n",
       "feature     tf_Ä often  tf_Ä example  tf_Ä said   tf_Ä all  tf_Ä while  tf_Ä first   \n",
       "\n",
       "                          204         205          206       207       208  \\\n",
       "importance           4.701181    4.675903     4.542944  4.366622  4.336646   \n",
       "feature     ts_mcalpine_eflaw  tf_Ä sports  tf_Ä helping   tf_Ä was  tf_Ä done   \n",
       "\n",
       "                 209           210          211       212       213  \\\n",
       "importance  4.335607      4.310486      4.16504  4.124278  4.074976   \n",
       "feature     tf_Ä last  tf_Ä feedback  tf_Ä believe  tf_Ä just    tf_Ä an   \n",
       "\n",
       "                   214          215       216       217         218  \\\n",
       "importance    4.058128     4.047112  4.033809  3.958862    3.883852   \n",
       "feature     tf_Ä people  tf_Ä another   tf_Ä why   tf_Ä who  tf_Ä reason   \n",
       "\n",
       "                     219          220       221       222       223       224  \\\n",
       "importance      3.794348     3.684112  3.624991  3.612345  3.540681  3.475764   \n",
       "feature     tf_Ä learning  tf_Ä schools   tf_Ä one    tf_Ä no  tf_Ä keep   tf_Ä but   \n",
       "\n",
       "                  225        226       227       228       229       230  \\\n",
       "importance   3.435429   3.407918  3.379809  3.342614  3.306682  3.263934   \n",
       "feature     tf_Ä going  tf_Ä about  tf_Ä were  tf_Ä than  tf_Ä stay   tf_Ä put   \n",
       "\n",
       "                 231             232       233       234         235  \\\n",
       "importance  3.259813        3.166806  3.154149  3.078102    2.938603   \n",
       "feature     tf_Ä good  va_valence_std    tf_Ä be  tf_Ä make  tf_Ä always   \n",
       "\n",
       "                 236       237       238       239       240       241  \\\n",
       "importance  2.877229  2.867808  2.849793  2.829164  2.787444   2.77756   \n",
       "feature      tf_Ä now   tf_Ä how   tf_Ä out    tf_Ä is  tf_Ä this  tf_Ä with   \n",
       "\n",
       "                 242       243        244                       245  \\\n",
       "importance  2.739072  2.726932   2.602082                  2.598131   \n",
       "feature       tf_Ä to   tf_Ä its  tf_Ä being  ts_linsear_write_formula   \n",
       "\n",
       "                   246       247       248         249        250       251  \\\n",
       "importance    2.557087  2.551506  2.533246    2.490699   2.484266  2.471044   \n",
       "feature     tf_Ä create   tf_Ä say  tf_Ä sure  tf_Ä person  tf_Ä using  tf_Ä down   \n",
       "\n",
       "                  252                  253      254       255       256  \\\n",
       "importance   2.448205             2.415814  2.31446  2.101922  2.099108   \n",
       "feature     tf_Ä which  ch_repeat_char_frac  tf_Ä for  tf_Ä same  tf_Ä take   \n",
       "\n",
       "                 257       258      259       260        261        262  \\\n",
       "importance  2.086792  2.078264  1.99505  1.982076   1.962839   1.936546   \n",
       "feature      tf_Ä are  tf_Ä more   tf_Ä in  tf_Ä when  tf_Ä there  tf_Ä cause   \n",
       "\n",
       "                   263       264          265         266        267  \\\n",
       "importance    1.923023  1.905253     1.903507    1.870733    1.86094   \n",
       "feature     tf_Ä others   tf_Ä fun  tf_Ä benefit  tf_Ä things  tf_Ä every   \n",
       "\n",
       "                  268               269        270       271        272  \\\n",
       "importance   1.827483          1.821375   1.767868  1.741594   1.710009   \n",
       "feature     tf_Ä doing  va_dominance_std  tf_Ä their  tf_Ä life  tf_Ä lives   \n",
       "\n",
       "                 273         274       275       276       277       278  \\\n",
       "importance  1.684256    1.683862  1.640992  1.616394  1.573892  1.500577   \n",
       "feature        tf_Ä s  tf_Ä during    tf_Ä up  tf_Ä have  tf_Ä part     tf_Ä a   \n",
       "\n",
       "                 279       280       281       282             283  \\\n",
       "importance  1.498889  1.496079   1.44943  1.437862        1.391529   \n",
       "feature     tf_Ä time  tf_Ä only  tf_Ä also  tf_Ä that  va_arousal_std   \n",
       "\n",
       "                      284         285       286       287       288  \\\n",
       "importance       1.379298    1.369891  1.338372  1.248961  1.240137   \n",
       "feature     tf_Ä different  tf_Ä around    tf_Ä or  tf_Ä them    tf_Ä on   \n",
       "\n",
       "                    289       290       291      292       293      294  \\\n",
       "importance     1.197424  1.115942  1.110137  1.07143  1.045055  1.04131   \n",
       "feature     tf_Ä instead  tf_Ä some    tf_Ä as  tf_Ä new    tf_Ä by   tf_Ä of   \n",
       "\n",
       "                 295        296       297       298         299         300  \\\n",
       "importance   1.03263   1.030561  1.013166  0.980068    0.973082    0.916443   \n",
       "feature     tf_Ä want  tf_Ä these  tf_Ä even   tf_Ä way  tf_Ä better  tf_Ä number   \n",
       "\n",
       "                 301        302       303       304       305       306  \\\n",
       "importance  0.817195   0.816088  0.770794  0.759886  0.702525  0.693356   \n",
       "feature     tf_Ä able  tf_Ä other   tf_Ä see  tf_Ä work  tf_Ä idea  tf_Ä from   \n",
       "\n",
       "                 307       308       309       310       311         312  \\\n",
       "importance  0.686387  0.643064  0.625346  0.590192  0.586884    0.576589   \n",
       "feature     tf_Ä such   tf_Ä any   tf_Ä lot  tf_Ä feel  tf_Ä look  tf_Ä future   \n",
       "\n",
       "                 313       314          315        316        317  \n",
       "importance  0.562961  0.556713     0.546316   0.301447   0.283508  \n",
       "feature     tf_Ä help  tf_Ä well  tf_Ä someone  tf_Ä great  tf_Ä takes  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores = model.get_score(importance_type=\"gain\")\n",
    "assert len(scores)!=0\n",
    "rows = []\n",
    "for feature, score in scores.items():\n",
    "    rows.append({'importance': score, 'feature': feature})\n",
    "idf = pd.DataFrame.from_records(rows)\n",
    "idf = idf.sort_values([\"importance\"], ascending=False, ignore_index=True)\n",
    "fp = f\"{job_dir}/importance.csv\"\n",
    "idf.to_csv(fp, index=True)\n",
    "print(f\"Saved {fp}\")\n",
    "idf.T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a994ecf2-2058-4117-923f-bda3a05f83f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken 0:17:29.462461\n"
     ]
    }
   ],
   "source": [
    "tim.stop()\n",
    "print(f\"Total time taken {str(tim.elapsed)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
