{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062a9473-ec3c-4f1c-80de-66912972bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from typing import List, Dict, Union, Tuple, NamedTuple\n",
    "from tqdm import tqdm\n",
    "import scml\n",
    "from scml import pandasx as pdx\n",
    "tim = scml.Timer()\n",
    "tim.start()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "percentiles=[.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99]\n",
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "tqdm.pandas()\n",
    "scml.seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafd5a38-0166-47b6-aed4-7ae66bd7bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "job_dir = f\"models/xgb/{ts}\"\n",
    "pathlib.Path(job_dir).mkdir(parents=True, exist_ok=True)\n",
    "num_boost_round: int = 100\n",
    "lr: Tuple[float, float] = (1e-3, 1e-3)\n",
    "feature_fraction: Tuple[float, float] = (1, 1)\n",
    "min_data_in_leaf: Tuple[int, int] = (20, 20)\n",
    "objective: str = \"binary:logistic\"\n",
    "n_trials: int = 1\n",
    "label = \"generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e822fad3-d0a9-4094-9efb-60cde058d454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39120 entries, 0 to 39119\n",
      "Columns: 29787 entries, essay_id to tf_Ġzygomatic\n",
      "dtypes: float32(29772), int16(2), int32(5), int8(1), object(7)\n",
      "memory usage: 4.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"input/features.parquet\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb8e82b-a582-4d2a-88e1-d4a4c85a1fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29775 features\n",
      "['ch_digit_frac', 'ch_len', 'ch_letter_frac', 'ch_punc_frac', 'ch_repeat_char_frac', 'ch_space_frac', 'ch_upper_frac', 'tf_0', 'tf_00', 'tf_000', 'tf_03', 'tf_1', 'tf_10', 'tf_11', 'tf_12', 'tf_13', 'tf_14', 'tf_15', 'tf_16', 'tf_17', 'tf_18', 'tf_19', 'tf_199', 'tf_1990', 'tf_2', 'tf_20', 'tf_200', 'tf_2002', 'tf_21', 'tf_23', 'tf_24', 'tf_25', 'tf_27', 'tf_28', 'tf_3', 'tf_30', 'tf_31', 'tf_32', 'tf_33', 'tf_34', 'tf_38', 'tf_39', 'tf_4', 'tf_40', 'tf_41', 'tf_43', 'tf_45', 'tf_5', 'tf_50', 'tf_538', 'tf_58', 'tf_6', 'tf_60', 'tf_62', 'tf_7', 'tf_70', 'tf_74', 'tf_76', 'tf_79', 'tf_8', 'tf_87', 'tf_9', 'tf_a', 'tf_aa', 'tf_aae', 'tf_aage', 'tf_aaion', 'tf_ab', 'tf_aban', 'tf_abe', 'tf_abel', 'tf_aber', 'tf_abet', 'tf_abeth', 'tf_abil', 'tf_abilites', 'tf_abilitie', 'tf_abilities', 'tf_ability', 'tf_abill', 'tf_abilty', 'tf_abitable', 'tf_abital', 'tf_abl', 'tf_able', 'tf_abled', 'tf_ables', 'tf_abling', 'tf_ablish', 'tf_ablished', 'tf_ablities', 'tf_ablity', 'tf_ably', 'tf_about', 'tf_abra', 'tf_abraeus', 'tf_abs', 'tf_abul', 'tf_aby', 'tf_ac']\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "prefixes = [\"ch_\", \"ws_\", \"ts_\", \"va_\", \"tf_\"]\n",
    "for col in df.columns:\n",
    "    for prefix in prefixes:\n",
    "        if col.startswith(prefix):\n",
    "            features.append(col)\n",
    "features.sort()\n",
    "print(f\"{len(features)} features\\n{features[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41fd2b91-247a-4891-9859-478bb1d42583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val%=0.0293, len(tra)=37,974, len(val)=1,146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generated</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>907</td>\n",
       "      <td>0.791449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>239</td>\n",
       "      <td>0.208551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count   percent\n",
       "generated                 \n",
       "1            907  0.791449\n",
       "0            239  0.208551"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(tra[features], tra[label], test_size=0.2)\n",
    "\n",
    "tra = df[df[\"white_sim\"]>=0.45]\n",
    "val = df[df[\"white_sim\"]<0.45]\n",
    "t = len(tra)\n",
    "v = len(val)\n",
    "n = t+v\n",
    "print(f\"val%={v/n:.4f}, len(tra)={t:,}, len(val)={v:,}\")\n",
    "dtrain = xgb.DMatrix(tra[features], tra[label], enable_categorical=False)\n",
    "dval = xgb.DMatrix(val[features], val[label], enable_categorical=False)\n",
    "pdx.value_counts(val[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df30c12e-601b-4dcb-8ab9-90f3beb13b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.51395\tval-logloss:1.07884\n",
      "[40]\ttrain-logloss:0.12450\tval-logloss:0.36778\n",
      "[80]\ttrain-logloss:0.05951\tval-logloss:0.22819\n",
      "[120]\ttrain-logloss:0.03711\tval-logloss:0.17022\n",
      "[160]\ttrain-logloss:0.02651\tval-logloss:0.13479\n",
      "[200]\ttrain-logloss:0.02038\tval-logloss:0.11529\n",
      "[240]\ttrain-logloss:0.01619\tval-logloss:0.10146\n",
      "[280]\ttrain-logloss:0.01334\tval-logloss:0.09335\n",
      "[320]\ttrain-logloss:0.01129\tval-logloss:0.08700\n",
      "[360]\ttrain-logloss:0.00969\tval-logloss:0.08254\n",
      "[400]\ttrain-logloss:0.00849\tval-logloss:0.07864\n",
      "[440]\ttrain-logloss:0.00752\tval-logloss:0.07500\n",
      "[480]\ttrain-logloss:0.00679\tval-logloss:0.07311\n",
      "[520]\ttrain-logloss:0.00621\tval-logloss:0.07109\n",
      "[560]\ttrain-logloss:0.00575\tval-logloss:0.07022\n",
      "[600]\ttrain-logloss:0.00536\tval-logloss:0.06864\n",
      "[640]\ttrain-logloss:0.00504\tval-logloss:0.06831\n",
      "[680]\ttrain-logloss:0.00476\tval-logloss:0.06800\n",
      "[720]\ttrain-logloss:0.00452\tval-logloss:0.06790\n",
      "[760]\ttrain-logloss:0.00433\tval-logloss:0.06766\n",
      "[800]\ttrain-logloss:0.00415\tval-logloss:0.06722\n",
      "[840]\ttrain-logloss:0.00399\tval-logloss:0.06677\n",
      "[880]\ttrain-logloss:0.00384\tval-logloss:0.06640\n",
      "[920]\ttrain-logloss:0.00372\tval-logloss:0.06595\n",
      "[960]\ttrain-logloss:0.00361\tval-logloss:0.06589\n",
      "[1000]\ttrain-logloss:0.00350\tval-logloss:0.06569\n",
      "[1040]\ttrain-logloss:0.00341\tval-logloss:0.06543\n",
      "[1080]\ttrain-logloss:0.00332\tval-logloss:0.06530\n",
      "[1120]\ttrain-logloss:0.00324\tval-logloss:0.06496\n",
      "[1160]\ttrain-logloss:0.00317\tval-logloss:0.06525\n",
      "[1200]\ttrain-logloss:0.00310\tval-logloss:0.06517\n",
      "[1218]\ttrain-logloss:0.00308\tval-logloss:0.06530\n",
      "best score 0.06493 at iteration 1119\n",
      "CPU times: user 3h 3min 47s, sys: 37min 26s, total: 3h 41min 13s\n",
      "Wall time: 17min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = xgb.train(\n",
    "   params={\n",
    "       \"objective\": objective,\n",
    "       \"learning_rate\": 5e-2,\n",
    "       \"min_child_weight\": 20,\n",
    "       \"colsample_bytree\": 0.5,\n",
    "       \"max_depth\": 6,\n",
    "   },\n",
    "   dtrain=dtrain,\n",
    "   num_boost_round=2000,\n",
    "   evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "   verbose_eval=40,\n",
    "   early_stopping_rounds=100,\n",
    ")\n",
    "print(f\"best score {model.best_score:.5f} at iteration {model.best_iteration}\")\n",
    "model.save_model(f\"{job_dir}/model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6da2370-9acb-42fa-894f-c4da29da6425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc=0.9978\n",
      "y_pred=(1146,)\n",
      "[0.9499928  0.97822237 0.99891245 0.00628967 0.99998033]\n",
      "CPU times: user 312 ms, sys: 400 ms, total: 712 ms\n",
      "Wall time: 59.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_true = val[label].tolist()\n",
    "y_pred = model.predict(data=dval, iteration_range=(0, model.best_iteration+1))\n",
    "auc = roc_auc_score(y_true, y_pred, average=\"macro\")\n",
    "print(f\"auc={auc:.4f}\")\n",
    "print(f\"y_pred={y_pred.shape}\\n{y_pred[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f793a39d-7bce-4472-aef3-bacde89ee929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models/xgb/20240120_041933/importance.csv\n",
      "CPU times: user 21.2 ms, sys: 38.7 ms, total: 59.9 ms\n",
      "Wall time: 5.05 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>importance</th>\n",
       "      <td>535.333862</td>\n",
       "      <td>491.513977</td>\n",
       "      <td>487.654358</td>\n",
       "      <td>362.805176</td>\n",
       "      <td>333.964478</td>\n",
       "      <td>332.914612</td>\n",
       "      <td>226.789703</td>\n",
       "      <td>219.15686</td>\n",
       "      <td>214.638794</td>\n",
       "      <td>211.520065</td>\n",
       "      <td>179.099335</td>\n",
       "      <td>171.318848</td>\n",
       "      <td>149.939621</td>\n",
       "      <td>144.979034</td>\n",
       "      <td>138.28273</td>\n",
       "      <td>124.150146</td>\n",
       "      <td>121.580658</td>\n",
       "      <td>115.805115</td>\n",
       "      <td>114.466164</td>\n",
       "      <td>108.677856</td>\n",
       "      <td>101.883476</td>\n",
       "      <td>101.435951</td>\n",
       "      <td>98.002876</td>\n",
       "      <td>84.509155</td>\n",
       "      <td>76.646042</td>\n",
       "      <td>76.45359</td>\n",
       "      <td>75.889709</td>\n",
       "      <td>69.939545</td>\n",
       "      <td>66.292404</td>\n",
       "      <td>66.05307</td>\n",
       "      <td>65.250473</td>\n",
       "      <td>63.459717</td>\n",
       "      <td>62.241436</td>\n",
       "      <td>56.666737</td>\n",
       "      <td>54.976513</td>\n",
       "      <td>54.656471</td>\n",
       "      <td>53.885494</td>\n",
       "      <td>53.665058</td>\n",
       "      <td>53.12447</td>\n",
       "      <td>52.420853</td>\n",
       "      <td>52.350555</td>\n",
       "      <td>51.53833</td>\n",
       "      <td>50.223473</td>\n",
       "      <td>49.986206</td>\n",
       "      <td>49.040386</td>\n",
       "      <td>47.444031</td>\n",
       "      <td>46.485104</td>\n",
       "      <td>46.267811</td>\n",
       "      <td>40.464008</td>\n",
       "      <td>40.103317</td>\n",
       "      <td>39.966915</td>\n",
       "      <td>38.075565</td>\n",
       "      <td>37.707947</td>\n",
       "      <td>37.261292</td>\n",
       "      <td>36.566113</td>\n",
       "      <td>36.080936</td>\n",
       "      <td>35.468941</td>\n",
       "      <td>33.705811</td>\n",
       "      <td>33.141094</td>\n",
       "      <td>30.172977</td>\n",
       "      <td>29.95191</td>\n",
       "      <td>29.405884</td>\n",
       "      <td>29.22114</td>\n",
       "      <td>28.309143</td>\n",
       "      <td>27.806349</td>\n",
       "      <td>27.620529</td>\n",
       "      <td>26.203878</td>\n",
       "      <td>26.058992</td>\n",
       "      <td>25.645138</td>\n",
       "      <td>25.579153</td>\n",
       "      <td>25.304052</td>\n",
       "      <td>24.991737</td>\n",
       "      <td>24.953922</td>\n",
       "      <td>24.29339</td>\n",
       "      <td>23.767916</td>\n",
       "      <td>22.674658</td>\n",
       "      <td>22.334621</td>\n",
       "      <td>21.925852</td>\n",
       "      <td>21.656654</td>\n",
       "      <td>21.583004</td>\n",
       "      <td>21.053091</td>\n",
       "      <td>20.341541</td>\n",
       "      <td>20.336012</td>\n",
       "      <td>19.925665</td>\n",
       "      <td>19.696354</td>\n",
       "      <td>19.651701</td>\n",
       "      <td>19.599421</td>\n",
       "      <td>19.536987</td>\n",
       "      <td>19.467497</td>\n",
       "      <td>19.375626</td>\n",
       "      <td>18.577335</td>\n",
       "      <td>18.273558</td>\n",
       "      <td>18.098648</td>\n",
       "      <td>18.076061</td>\n",
       "      <td>17.982529</td>\n",
       "      <td>17.886499</td>\n",
       "      <td>17.813538</td>\n",
       "      <td>17.708664</td>\n",
       "      <td>17.696554</td>\n",
       "      <td>17.532536</td>\n",
       "      <td>17.430914</td>\n",
       "      <td>17.187277</td>\n",
       "      <td>17.073383</td>\n",
       "      <td>16.762617</td>\n",
       "      <td>16.444614</td>\n",
       "      <td>16.400848</td>\n",
       "      <td>16.220268</td>\n",
       "      <td>16.089945</td>\n",
       "      <td>16.070419</td>\n",
       "      <td>15.904037</td>\n",
       "      <td>15.884532</td>\n",
       "      <td>15.785007</td>\n",
       "      <td>15.684215</td>\n",
       "      <td>14.927987</td>\n",
       "      <td>14.50132</td>\n",
       "      <td>14.360294</td>\n",
       "      <td>14.077721</td>\n",
       "      <td>14.069798</td>\n",
       "      <td>14.040625</td>\n",
       "      <td>13.981558</td>\n",
       "      <td>13.925736</td>\n",
       "      <td>13.558373</td>\n",
       "      <td>13.070453</td>\n",
       "      <td>13.039894</td>\n",
       "      <td>13.004128</td>\n",
       "      <td>12.855943</td>\n",
       "      <td>12.818004</td>\n",
       "      <td>12.804463</td>\n",
       "      <td>12.793447</td>\n",
       "      <td>12.426529</td>\n",
       "      <td>12.19511</td>\n",
       "      <td>12.099344</td>\n",
       "      <td>12.074789</td>\n",
       "      <td>12.037127</td>\n",
       "      <td>11.92334</td>\n",
       "      <td>11.868005</td>\n",
       "      <td>11.733463</td>\n",
       "      <td>11.649146</td>\n",
       "      <td>11.290364</td>\n",
       "      <td>11.227482</td>\n",
       "      <td>10.756605</td>\n",
       "      <td>10.752293</td>\n",
       "      <td>10.73936</td>\n",
       "      <td>10.539305</td>\n",
       "      <td>10.441528</td>\n",
       "      <td>10.283181</td>\n",
       "      <td>10.095117</td>\n",
       "      <td>9.960876</td>\n",
       "      <td>9.8633</td>\n",
       "      <td>9.83662</td>\n",
       "      <td>9.826649</td>\n",
       "      <td>9.656018</td>\n",
       "      <td>9.445007</td>\n",
       "      <td>9.382985</td>\n",
       "      <td>9.347935</td>\n",
       "      <td>9.164958</td>\n",
       "      <td>9.125072</td>\n",
       "      <td>8.971872</td>\n",
       "      <td>8.706679</td>\n",
       "      <td>8.687664</td>\n",
       "      <td>8.57763</td>\n",
       "      <td>8.3493</td>\n",
       "      <td>8.201652</td>\n",
       "      <td>7.949665</td>\n",
       "      <td>7.875024</td>\n",
       "      <td>7.80212</td>\n",
       "      <td>7.797144</td>\n",
       "      <td>7.544501</td>\n",
       "      <td>7.445221</td>\n",
       "      <td>7.244454</td>\n",
       "      <td>7.150042</td>\n",
       "      <td>6.944552</td>\n",
       "      <td>6.941762</td>\n",
       "      <td>6.930393</td>\n",
       "      <td>6.874111</td>\n",
       "      <td>6.838174</td>\n",
       "      <td>6.832476</td>\n",
       "      <td>6.773543</td>\n",
       "      <td>6.651915</td>\n",
       "      <td>6.580213</td>\n",
       "      <td>6.485192</td>\n",
       "      <td>6.468338</td>\n",
       "      <td>6.456057</td>\n",
       "      <td>6.327619</td>\n",
       "      <td>6.102661</td>\n",
       "      <td>6.045862</td>\n",
       "      <td>6.010983</td>\n",
       "      <td>5.981054</td>\n",
       "      <td>5.956406</td>\n",
       "      <td>5.908396</td>\n",
       "      <td>5.868488</td>\n",
       "      <td>5.848146</td>\n",
       "      <td>5.813245</td>\n",
       "      <td>5.774518</td>\n",
       "      <td>5.641491</td>\n",
       "      <td>5.617348</td>\n",
       "      <td>5.562448</td>\n",
       "      <td>5.496105</td>\n",
       "      <td>5.446288</td>\n",
       "      <td>4.977261</td>\n",
       "      <td>4.940655</td>\n",
       "      <td>4.803391</td>\n",
       "      <td>4.802198</td>\n",
       "      <td>4.739406</td>\n",
       "      <td>4.701181</td>\n",
       "      <td>4.675903</td>\n",
       "      <td>4.542944</td>\n",
       "      <td>4.366622</td>\n",
       "      <td>4.336646</td>\n",
       "      <td>4.335607</td>\n",
       "      <td>4.310486</td>\n",
       "      <td>4.16504</td>\n",
       "      <td>4.124278</td>\n",
       "      <td>4.074976</td>\n",
       "      <td>4.058128</td>\n",
       "      <td>4.047112</td>\n",
       "      <td>4.033809</td>\n",
       "      <td>3.958862</td>\n",
       "      <td>3.883852</td>\n",
       "      <td>3.794348</td>\n",
       "      <td>3.684112</td>\n",
       "      <td>3.624991</td>\n",
       "      <td>3.612345</td>\n",
       "      <td>3.540681</td>\n",
       "      <td>3.475764</td>\n",
       "      <td>3.435429</td>\n",
       "      <td>3.407918</td>\n",
       "      <td>3.379809</td>\n",
       "      <td>3.342614</td>\n",
       "      <td>3.306682</td>\n",
       "      <td>3.263934</td>\n",
       "      <td>3.259813</td>\n",
       "      <td>3.166806</td>\n",
       "      <td>3.154149</td>\n",
       "      <td>3.078102</td>\n",
       "      <td>2.938603</td>\n",
       "      <td>2.877229</td>\n",
       "      <td>2.867808</td>\n",
       "      <td>2.849793</td>\n",
       "      <td>2.829164</td>\n",
       "      <td>2.787444</td>\n",
       "      <td>2.77756</td>\n",
       "      <td>2.739072</td>\n",
       "      <td>2.726932</td>\n",
       "      <td>2.602082</td>\n",
       "      <td>2.598131</td>\n",
       "      <td>2.557087</td>\n",
       "      <td>2.551506</td>\n",
       "      <td>2.533246</td>\n",
       "      <td>2.490699</td>\n",
       "      <td>2.484266</td>\n",
       "      <td>2.471044</td>\n",
       "      <td>2.448205</td>\n",
       "      <td>2.415814</td>\n",
       "      <td>2.31446</td>\n",
       "      <td>2.101922</td>\n",
       "      <td>2.099108</td>\n",
       "      <td>2.086792</td>\n",
       "      <td>2.078264</td>\n",
       "      <td>1.99505</td>\n",
       "      <td>1.982076</td>\n",
       "      <td>1.962839</td>\n",
       "      <td>1.936546</td>\n",
       "      <td>1.923023</td>\n",
       "      <td>1.905253</td>\n",
       "      <td>1.903507</td>\n",
       "      <td>1.870733</td>\n",
       "      <td>1.86094</td>\n",
       "      <td>1.827483</td>\n",
       "      <td>1.821375</td>\n",
       "      <td>1.767868</td>\n",
       "      <td>1.741594</td>\n",
       "      <td>1.710009</td>\n",
       "      <td>1.684256</td>\n",
       "      <td>1.683862</td>\n",
       "      <td>1.640992</td>\n",
       "      <td>1.616394</td>\n",
       "      <td>1.573892</td>\n",
       "      <td>1.500577</td>\n",
       "      <td>1.498889</td>\n",
       "      <td>1.496079</td>\n",
       "      <td>1.44943</td>\n",
       "      <td>1.437862</td>\n",
       "      <td>1.391529</td>\n",
       "      <td>1.379298</td>\n",
       "      <td>1.369891</td>\n",
       "      <td>1.338372</td>\n",
       "      <td>1.248961</td>\n",
       "      <td>1.240137</td>\n",
       "      <td>1.197424</td>\n",
       "      <td>1.115942</td>\n",
       "      <td>1.110137</td>\n",
       "      <td>1.07143</td>\n",
       "      <td>1.045055</td>\n",
       "      <td>1.04131</td>\n",
       "      <td>1.03263</td>\n",
       "      <td>1.030561</td>\n",
       "      <td>1.013166</td>\n",
       "      <td>0.980068</td>\n",
       "      <td>0.973082</td>\n",
       "      <td>0.916443</td>\n",
       "      <td>0.817195</td>\n",
       "      <td>0.816088</td>\n",
       "      <td>0.770794</td>\n",
       "      <td>0.759886</td>\n",
       "      <td>0.702525</td>\n",
       "      <td>0.693356</td>\n",
       "      <td>0.686387</td>\n",
       "      <td>0.643064</td>\n",
       "      <td>0.625346</td>\n",
       "      <td>0.590192</td>\n",
       "      <td>0.586884</td>\n",
       "      <td>0.576589</td>\n",
       "      <td>0.562961</td>\n",
       "      <td>0.556713</td>\n",
       "      <td>0.546316</td>\n",
       "      <td>0.301447</td>\n",
       "      <td>0.283508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <td>ts_polysyllable_frac</td>\n",
       "      <td>ts_syllables_per_word</td>\n",
       "      <td>tf_Ġhey</td>\n",
       "      <td>tf_Ġperformance</td>\n",
       "      <td>tf_Ġsuper</td>\n",
       "      <td>ch_space_frac</td>\n",
       "      <td>tf_Ġessay</td>\n",
       "      <td>tf_Ġensures</td>\n",
       "      <td>ts_flesch_kincaid_grade</td>\n",
       "      <td>tf_Ġgoals</td>\n",
       "      <td>tf_Ġessential</td>\n",
       "      <td>tf_Ġtotally</td>\n",
       "      <td>tf_Ġbecause</td>\n",
       "      <td>tf_th</td>\n",
       "      <td>tf_Ġadditionally</td>\n",
       "      <td>tf_Ġultimately</td>\n",
       "      <td>tf_Ġwould</td>\n",
       "      <td>tf_Ġimportance</td>\n",
       "      <td>tf_Ġgrader</td>\n",
       "      <td>tf_Ġstuff</td>\n",
       "      <td>tf_Ġfirstly</td>\n",
       "      <td>tf_Ġnt</td>\n",
       "      <td>tf_Ġvery</td>\n",
       "      <td>tf_Ġattempt</td>\n",
       "      <td>ch_punc_frac</td>\n",
       "      <td>ts_coleman_liau_index</td>\n",
       "      <td>tf_Ġsustainable</td>\n",
       "      <td>tf_Ġsuccess</td>\n",
       "      <td>tf_Ġconclusion</td>\n",
       "      <td>ch_letter_frac</td>\n",
       "      <td>tf_Ġchina</td>\n",
       "      <td>tf_Ġpresident</td>\n",
       "      <td>tf_Ġand</td>\n",
       "      <td>tf_Ġfacial</td>\n",
       "      <td>tf_Ġvenus</td>\n",
       "      <td>tf_Ġthank</td>\n",
       "      <td>ws_sent_len_std</td>\n",
       "      <td>tf_Ġimportant</td>\n",
       "      <td>ts_smog_index</td>\n",
       "      <td>tf_Ġlike</td>\n",
       "      <td>tf_Ġaddress</td>\n",
       "      <td>tf_Ġeurope</td>\n",
       "      <td>tf_Ġanimals</td>\n",
       "      <td>tf_Ġperspectives</td>\n",
       "      <td>tf_Ġcomputer</td>\n",
       "      <td>tf_Ġdriverless</td>\n",
       "      <td>ws_sent_len_delta_mean</td>\n",
       "      <td>tf_Ġ8</td>\n",
       "      <td>ts_lexicon_count</td>\n",
       "      <td>tf_Ġteacher</td>\n",
       "      <td>tf_Ġpotential</td>\n",
       "      <td>tf_Ġextracurricular</td>\n",
       "      <td>tf_Ġdear</td>\n",
       "      <td>ch_digit_frac</td>\n",
       "      <td>tf_Ġinformed</td>\n",
       "      <td>tf_Ġsignificant</td>\n",
       "      <td>tf_Ġseagoing</td>\n",
       "      <td>tf_Ġachieve</td>\n",
       "      <td>tf_Ġcar</td>\n",
       "      <td>ws_sent_len_delta_std</td>\n",
       "      <td>tf_Ġalthough</td>\n",
       "      <td>tf_Ġnasa</td>\n",
       "      <td>tf_Ġelectors</td>\n",
       "      <td>tf_Ġphone</td>\n",
       "      <td>tf_Ġthen</td>\n",
       "      <td>tf_Ġadvantages</td>\n",
       "      <td>tf_Ġsmaller</td>\n",
       "      <td>tf_Ġwriting</td>\n",
       "      <td>tf_Ġactivity</td>\n",
       "      <td>tf_Ġhumans</td>\n",
       "      <td>tf_Ġsecondly</td>\n",
       "      <td>tf_Ġcool</td>\n",
       "      <td>ts_monosyllable_frac</td>\n",
       "      <td>tf_Ġplus</td>\n",
       "      <td>tf_Ġskills</td>\n",
       "      <td>tf_Ġhuman</td>\n",
       "      <td>tf_Ġus</td>\n",
       "      <td>tf_Ġprovide</td>\n",
       "      <td>tf_Ġoverall</td>\n",
       "      <td>tf_Ġensure</td>\n",
       "      <td>tf_Ġexperiences</td>\n",
       "      <td>ts_dale_chall_readability_score</td>\n",
       "      <td>tf_Ġalmost</td>\n",
       "      <td>tf_Ġparagraph</td>\n",
       "      <td>tf_Ġprobably</td>\n",
       "      <td>tf_Ġdo</td>\n",
       "      <td>tf_Ġdriving</td>\n",
       "      <td>tf_Ġcareer</td>\n",
       "      <td>ts_automated_readability_index</td>\n",
       "      <td>tf_Ġreducing</td>\n",
       "      <td>tf_Ġcars</td>\n",
       "      <td>tf_Ġunique</td>\n",
       "      <td>tf_Ġhand</td>\n",
       "      <td>tf_Ġphones</td>\n",
       "      <td>tf_Ġlead</td>\n",
       "      <td>tf_Ġconsider</td>\n",
       "      <td>tf_Ġallows</td>\n",
       "      <td>ch_len</td>\n",
       "      <td>tf_Ġresources</td>\n",
       "      <td>tf_Ġleast</td>\n",
       "      <td>tf_Ġpercent</td>\n",
       "      <td>tf_Ġyou</td>\n",
       "      <td>tf_Ġeveryday</td>\n",
       "      <td>tf_Ġargue</td>\n",
       "      <td>tf_Ġtext</td>\n",
       "      <td>tf_Ġreduce</td>\n",
       "      <td>tf_Ġservice</td>\n",
       "      <td>ts_sentence_count</td>\n",
       "      <td>tf_Ġsmog</td>\n",
       "      <td>tf_Ġlet</td>\n",
       "      <td>tf_Ġthe</td>\n",
       "      <td>tf_Ġfurthermore</td>\n",
       "      <td>tf_Ġfair</td>\n",
       "      <td>tf_Ġgo</td>\n",
       "      <td>tf_Ġgrade</td>\n",
       "      <td>tf_Ġday</td>\n",
       "      <td>tf_Ġmany</td>\n",
       "      <td>tf_Ġprincipal</td>\n",
       "      <td>ts_difficult_words</td>\n",
       "      <td>tf_Ġcommunity</td>\n",
       "      <td>tf_Ġdifficult</td>\n",
       "      <td>tf_Ġsense</td>\n",
       "      <td>tf_Ġstates</td>\n",
       "      <td>ts_gunning_fog</td>\n",
       "      <td>tf_Ġexplore</td>\n",
       "      <td>tf_Ġagree</td>\n",
       "      <td>va_valence_mean</td>\n",
       "      <td>tf_Ġmight</td>\n",
       "      <td>tf_Ġso</td>\n",
       "      <td>tf_Ġthough</td>\n",
       "      <td>tf_Ġwill</td>\n",
       "      <td>tf_Ġearth</td>\n",
       "      <td>tf_Ġmost</td>\n",
       "      <td>tf_Ġfinally</td>\n",
       "      <td>tf_Ġcomputers</td>\n",
       "      <td>tf_Ġsincerely</td>\n",
       "      <td>tf_Ġexperience</td>\n",
       "      <td>tf_Ġtrue</td>\n",
       "      <td>tf_Ġstudents</td>\n",
       "      <td>tf_Ġreally</td>\n",
       "      <td>tf_Ġhealth</td>\n",
       "      <td>tf_Ġi</td>\n",
       "      <td>tf_Ġschool</td>\n",
       "      <td>tf_Ġlearn</td>\n",
       "      <td>tf_Ġpublic</td>\n",
       "      <td>tf_Ġtechnology</td>\n",
       "      <td>tf_Ġsupport</td>\n",
       "      <td>tf_Ġdue</td>\n",
       "      <td>tf_Ġmy</td>\n",
       "      <td>tf_Ġtransportation</td>\n",
       "      <td>ts_spache_readability</td>\n",
       "      <td>tf_Ġif</td>\n",
       "      <td>tf_Ġrequired</td>\n",
       "      <td>tf_Ġstudent</td>\n",
       "      <td>tf_Ġcould</td>\n",
       "      <td>ch_upper_frac</td>\n",
       "      <td>tf_Ġimpact</td>\n",
       "      <td>tf_Ġname</td>\n",
       "      <td>tf_Ġafter</td>\n",
       "      <td>va_arousal_mean</td>\n",
       "      <td>tf_Ġbenefits</td>\n",
       "      <td>tf_Ġboth</td>\n",
       "      <td>ts_words_per_sent</td>\n",
       "      <td>tf_Ġwhat</td>\n",
       "      <td>tf_Ġshould</td>\n",
       "      <td>tf_Ġmuch</td>\n",
       "      <td>tf_Ġreasons</td>\n",
       "      <td>tf_Ġmeans</td>\n",
       "      <td>tf_Ġlimiting</td>\n",
       "      <td>ts_syllable_count</td>\n",
       "      <td>tf_Ġnot</td>\n",
       "      <td>tf_Ġour</td>\n",
       "      <td>ts_syllables_per_sent</td>\n",
       "      <td>tf_Ġyour</td>\n",
       "      <td>ts_flesch_reading_ease</td>\n",
       "      <td>tf_Ġam</td>\n",
       "      <td>tf_Ġpoint</td>\n",
       "      <td>tf_Ġit</td>\n",
       "      <td>tf_Ġthey</td>\n",
       "      <td>tf_Ġknow</td>\n",
       "      <td>tf_Ġat</td>\n",
       "      <td>tf_Ġget</td>\n",
       "      <td>va_dominance_mean</td>\n",
       "      <td>tf_Ġfocus</td>\n",
       "      <td>tf_Ġown</td>\n",
       "      <td>ws_sent_len_mean</td>\n",
       "      <td>tf_Ġeverything</td>\n",
       "      <td>tf_Ġkids</td>\n",
       "      <td>tf_Ġwe</td>\n",
       "      <td>tf_Ġhard</td>\n",
       "      <td>tf_Ġover</td>\n",
       "      <td>tf_Ġhome</td>\n",
       "      <td>tf_Ġthink</td>\n",
       "      <td>tf_Ġmay</td>\n",
       "      <td>tf_Ġcan</td>\n",
       "      <td>tf_Ġstudying</td>\n",
       "      <td>tf_Ġthing</td>\n",
       "      <td>tf_Ġtwo</td>\n",
       "      <td>tf_Ġoften</td>\n",
       "      <td>tf_Ġexample</td>\n",
       "      <td>tf_Ġsaid</td>\n",
       "      <td>tf_Ġall</td>\n",
       "      <td>tf_Ġwhile</td>\n",
       "      <td>tf_Ġfirst</td>\n",
       "      <td>ts_mcalpine_eflaw</td>\n",
       "      <td>tf_Ġsports</td>\n",
       "      <td>tf_Ġhelping</td>\n",
       "      <td>tf_Ġwas</td>\n",
       "      <td>tf_Ġdone</td>\n",
       "      <td>tf_Ġlast</td>\n",
       "      <td>tf_Ġfeedback</td>\n",
       "      <td>tf_Ġbelieve</td>\n",
       "      <td>tf_Ġjust</td>\n",
       "      <td>tf_Ġan</td>\n",
       "      <td>tf_Ġpeople</td>\n",
       "      <td>tf_Ġanother</td>\n",
       "      <td>tf_Ġwhy</td>\n",
       "      <td>tf_Ġwho</td>\n",
       "      <td>tf_Ġreason</td>\n",
       "      <td>tf_Ġlearning</td>\n",
       "      <td>tf_Ġschools</td>\n",
       "      <td>tf_Ġone</td>\n",
       "      <td>tf_Ġno</td>\n",
       "      <td>tf_Ġkeep</td>\n",
       "      <td>tf_Ġbut</td>\n",
       "      <td>tf_Ġgoing</td>\n",
       "      <td>tf_Ġabout</td>\n",
       "      <td>tf_Ġwere</td>\n",
       "      <td>tf_Ġthan</td>\n",
       "      <td>tf_Ġstay</td>\n",
       "      <td>tf_Ġput</td>\n",
       "      <td>tf_Ġgood</td>\n",
       "      <td>va_valence_std</td>\n",
       "      <td>tf_Ġbe</td>\n",
       "      <td>tf_Ġmake</td>\n",
       "      <td>tf_Ġalways</td>\n",
       "      <td>tf_Ġnow</td>\n",
       "      <td>tf_Ġhow</td>\n",
       "      <td>tf_Ġout</td>\n",
       "      <td>tf_Ġis</td>\n",
       "      <td>tf_Ġthis</td>\n",
       "      <td>tf_Ġwith</td>\n",
       "      <td>tf_Ġto</td>\n",
       "      <td>tf_Ġits</td>\n",
       "      <td>tf_Ġbeing</td>\n",
       "      <td>ts_linsear_write_formula</td>\n",
       "      <td>tf_Ġcreate</td>\n",
       "      <td>tf_Ġsay</td>\n",
       "      <td>tf_Ġsure</td>\n",
       "      <td>tf_Ġperson</td>\n",
       "      <td>tf_Ġusing</td>\n",
       "      <td>tf_Ġdown</td>\n",
       "      <td>tf_Ġwhich</td>\n",
       "      <td>ch_repeat_char_frac</td>\n",
       "      <td>tf_Ġfor</td>\n",
       "      <td>tf_Ġsame</td>\n",
       "      <td>tf_Ġtake</td>\n",
       "      <td>tf_Ġare</td>\n",
       "      <td>tf_Ġmore</td>\n",
       "      <td>tf_Ġin</td>\n",
       "      <td>tf_Ġwhen</td>\n",
       "      <td>tf_Ġthere</td>\n",
       "      <td>tf_Ġcause</td>\n",
       "      <td>tf_Ġothers</td>\n",
       "      <td>tf_Ġfun</td>\n",
       "      <td>tf_Ġbenefit</td>\n",
       "      <td>tf_Ġthings</td>\n",
       "      <td>tf_Ġevery</td>\n",
       "      <td>tf_Ġdoing</td>\n",
       "      <td>va_dominance_std</td>\n",
       "      <td>tf_Ġtheir</td>\n",
       "      <td>tf_Ġlife</td>\n",
       "      <td>tf_Ġlives</td>\n",
       "      <td>tf_Ġs</td>\n",
       "      <td>tf_Ġduring</td>\n",
       "      <td>tf_Ġup</td>\n",
       "      <td>tf_Ġhave</td>\n",
       "      <td>tf_Ġpart</td>\n",
       "      <td>tf_Ġa</td>\n",
       "      <td>tf_Ġtime</td>\n",
       "      <td>tf_Ġonly</td>\n",
       "      <td>tf_Ġalso</td>\n",
       "      <td>tf_Ġthat</td>\n",
       "      <td>va_arousal_std</td>\n",
       "      <td>tf_Ġdifferent</td>\n",
       "      <td>tf_Ġaround</td>\n",
       "      <td>tf_Ġor</td>\n",
       "      <td>tf_Ġthem</td>\n",
       "      <td>tf_Ġon</td>\n",
       "      <td>tf_Ġinstead</td>\n",
       "      <td>tf_Ġsome</td>\n",
       "      <td>tf_Ġas</td>\n",
       "      <td>tf_Ġnew</td>\n",
       "      <td>tf_Ġby</td>\n",
       "      <td>tf_Ġof</td>\n",
       "      <td>tf_Ġwant</td>\n",
       "      <td>tf_Ġthese</td>\n",
       "      <td>tf_Ġeven</td>\n",
       "      <td>tf_Ġway</td>\n",
       "      <td>tf_Ġbetter</td>\n",
       "      <td>tf_Ġnumber</td>\n",
       "      <td>tf_Ġable</td>\n",
       "      <td>tf_Ġother</td>\n",
       "      <td>tf_Ġsee</td>\n",
       "      <td>tf_Ġwork</td>\n",
       "      <td>tf_Ġidea</td>\n",
       "      <td>tf_Ġfrom</td>\n",
       "      <td>tf_Ġsuch</td>\n",
       "      <td>tf_Ġany</td>\n",
       "      <td>tf_Ġlot</td>\n",
       "      <td>tf_Ġfeel</td>\n",
       "      <td>tf_Ġlook</td>\n",
       "      <td>tf_Ġfuture</td>\n",
       "      <td>tf_Ġhelp</td>\n",
       "      <td>tf_Ġwell</td>\n",
       "      <td>tf_Ġsomeone</td>\n",
       "      <td>tf_Ġgreat</td>\n",
       "      <td>tf_Ġtakes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0                      1           2    \\\n",
       "importance            535.333862             491.513977  487.654358   \n",
       "feature     ts_polysyllable_frac  ts_syllables_per_word     tf_Ġhey   \n",
       "\n",
       "                        3           4              5           6    \\\n",
       "importance       362.805176  333.964478     332.914612  226.789703   \n",
       "feature     tf_Ġperformance   tf_Ġsuper  ch_space_frac   tf_Ġessay   \n",
       "\n",
       "                    7                        8           9              10   \\\n",
       "importance    219.15686               214.638794  211.520065     179.099335   \n",
       "feature     tf_Ġensures  ts_flesch_kincaid_grade   tf_Ġgoals  tf_Ġessential   \n",
       "\n",
       "                    11           12          13                14   \\\n",
       "importance   171.318848   149.939621  144.979034         138.28273   \n",
       "feature     tf_Ġtotally  tf_Ġbecause       tf_th  tf_Ġadditionally   \n",
       "\n",
       "                       15          16              17          18   \\\n",
       "importance      124.150146  121.580658      115.805115  114.466164   \n",
       "feature     tf_Ġultimately   tf_Ġwould  tf_Ġimportance  tf_Ġgrader   \n",
       "\n",
       "                   19           20          21         22           23   \\\n",
       "importance  108.677856   101.883476  101.435951  98.002876    84.509155   \n",
       "feature      tf_Ġstuff  tf_Ġfirstly      tf_Ġnt   tf_Ġvery  tf_Ġattempt   \n",
       "\n",
       "                     24                     25               26           27   \\\n",
       "importance     76.646042               76.45359        75.889709    69.939545   \n",
       "feature     ch_punc_frac  ts_coleman_liau_index  tf_Ġsustainable  tf_Ġsuccess   \n",
       "\n",
       "                       28              29         30             31   \\\n",
       "importance       66.292404        66.05307  65.250473      63.459717   \n",
       "feature     tf_Ġconclusion  ch_letter_frac  tf_Ġchina  tf_Ġpresident   \n",
       "\n",
       "                  32          33         34         35               36   \\\n",
       "importance  62.241436   56.666737  54.976513  54.656471        53.885494   \n",
       "feature       tf_Ġand  tf_Ġfacial  tf_Ġvenus  tf_Ġthank  ws_sent_len_std   \n",
       "\n",
       "                      37             38         39           40          41   \\\n",
       "importance      53.665058       53.12447  52.420853    52.350555    51.53833   \n",
       "feature     tf_Ġimportant  ts_smog_index   tf_Ġlike  tf_Ġaddress  tf_Ġeurope   \n",
       "\n",
       "                    42                43            44              45   \\\n",
       "importance    50.223473         49.986206     49.040386       47.444031   \n",
       "feature     tf_Ġanimals  tf_Ġperspectives  tf_Ġcomputer  tf_Ġdriverless   \n",
       "\n",
       "                               46         47                48           49   \\\n",
       "importance               46.485104  46.267811         40.464008    40.103317   \n",
       "feature     ws_sent_len_delta_mean      tf_Ġ8  ts_lexicon_count  tf_Ġteacher   \n",
       "\n",
       "                      50                   51         52             53   \\\n",
       "importance      39.966915            38.075565  37.707947      37.261292   \n",
       "feature     tf_Ġpotential  tf_Ġextracurricular   tf_Ġdear  ch_digit_frac   \n",
       "\n",
       "                     54               55            56           57   \\\n",
       "importance     36.566113        36.080936     35.468941    33.705811   \n",
       "feature     tf_Ġinformed  tf_Ġsignificant  tf_Ġseagoing  tf_Ġachieve   \n",
       "\n",
       "                  58                     59            60         61   \\\n",
       "importance  33.141094              30.172977      29.95191  29.405884   \n",
       "feature       tf_Ġcar  ws_sent_len_delta_std  tf_Ġalthough   tf_Ġnasa   \n",
       "\n",
       "                     62         63         64              65           66   \\\n",
       "importance      29.22114  28.309143  27.806349       27.620529    26.203878   \n",
       "feature     tf_Ġelectors  tf_Ġphone   tf_Ġthen  tf_Ġadvantages  tf_Ġsmaller   \n",
       "\n",
       "                    67            68          69            70         71   \\\n",
       "importance    26.058992     25.645138   25.579153     25.304052  24.991737   \n",
       "feature     tf_Ġwriting  tf_Ġactivity  tf_Ġhumans  tf_Ġsecondly   tf_Ġcool   \n",
       "\n",
       "                             72        73          74         75         76   \\\n",
       "importance             24.953922  24.29339   23.767916  22.674658  22.334621   \n",
       "feature     ts_monosyllable_frac  tf_Ġplus  tf_Ġskills  tf_Ġhuman     tf_Ġus   \n",
       "\n",
       "                    77           78          79               80   \\\n",
       "importance    21.925852    21.656654   21.583004        21.053091   \n",
       "feature     tf_Ġprovide  tf_Ġoverall  tf_Ġensure  tf_Ġexperiences   \n",
       "\n",
       "                                        81          82             83   \\\n",
       "importance                        20.341541   20.336012      19.925665   \n",
       "feature     ts_dale_chall_readability_score  tf_Ġalmost  tf_Ġparagraph   \n",
       "\n",
       "                     84         85           86          87   \\\n",
       "importance     19.696354  19.651701    19.599421   19.536987   \n",
       "feature     tf_Ġprobably     tf_Ġdo  tf_Ġdriving  tf_Ġcareer   \n",
       "\n",
       "                                       88            89         90   \\\n",
       "importance                       19.467497     19.375626  18.577335   \n",
       "feature     ts_automated_readability_index  tf_Ġreducing   tf_Ġcars   \n",
       "\n",
       "                   91         92          93         94            95   \\\n",
       "importance   18.273558  18.098648   18.076061  17.982529     17.886499   \n",
       "feature     tf_Ġunique   tf_Ġhand  tf_Ġphones   tf_Ġlead  tf_Ġconsider   \n",
       "\n",
       "                   96         97             98         99           100  \\\n",
       "importance   17.813538  17.708664      17.696554  17.532536    17.430914   \n",
       "feature     tf_Ġallows     ch_len  tf_Ġresources  tf_Ġleast  tf_Ġpercent   \n",
       "\n",
       "                  101           102        103        104         105  \\\n",
       "importance  17.187277     17.073383  16.762617  16.444614   16.400848   \n",
       "feature       tf_Ġyou  tf_Ġeveryday  tf_Ġargue   tf_Ġtext  tf_Ġreduce   \n",
       "\n",
       "                    106                107        108        109        110  \\\n",
       "importance    16.220268          16.089945  16.070419  15.904037  15.884532   \n",
       "feature     tf_Ġservice  ts_sentence_count   tf_Ġsmog    tf_Ġlet    tf_Ġthe   \n",
       "\n",
       "                        111        112        113        114        115  \\\n",
       "importance        15.785007  15.684215  14.927987   14.50132  14.360294   \n",
       "feature     tf_Ġfurthermore   tf_Ġfair     tf_Ġgo  tf_Ġgrade    tf_Ġday   \n",
       "\n",
       "                  116            117                 118            119  \\\n",
       "importance  14.077721      14.069798           14.040625      13.981558   \n",
       "feature      tf_Ġmany  tf_Ġprincipal  ts_difficult_words  tf_Ġcommunity   \n",
       "\n",
       "                      120        121         122             123          124  \\\n",
       "importance      13.925736  13.558373   13.070453       13.039894    13.004128   \n",
       "feature     tf_Ġdifficult  tf_Ġsense  tf_Ġstates  ts_gunning_fog  tf_Ġexplore   \n",
       "\n",
       "                  125              126        127        128         129  \\\n",
       "importance  12.855943        12.818004  12.804463  12.793447   12.426529   \n",
       "feature     tf_Ġagree  va_valence_mean  tf_Ġmight     tf_Ġso  tf_Ġthough   \n",
       "\n",
       "                 130        131        132          133            134  \\\n",
       "importance  12.19511  12.099344  12.074789    12.037127       11.92334   \n",
       "feature     tf_Ġwill  tf_Ġearth   tf_Ġmost  tf_Ġfinally  tf_Ġcomputers   \n",
       "\n",
       "                      135             136        137           138  \\\n",
       "importance      11.868005       11.733463  11.649146     11.290364   \n",
       "feature     tf_Ġsincerely  tf_Ġexperience   tf_Ġtrue  tf_Ġstudents   \n",
       "\n",
       "                   139         140        141         142        143  \\\n",
       "importance   11.227482   10.756605  10.752293    10.73936  10.539305   \n",
       "feature     tf_Ġreally  tf_Ġhealth      tf_Ġi  tf_Ġschool  tf_Ġlearn   \n",
       "\n",
       "                   144             145          146       147     148  \\\n",
       "importance   10.441528       10.283181    10.095117  9.960876  9.8633   \n",
       "feature     tf_Ġpublic  tf_Ġtechnology  tf_Ġsupport   tf_Ġdue  tf_Ġmy   \n",
       "\n",
       "                           149                    150       151           152  \\\n",
       "importance             9.83662               9.826649  9.656018      9.445007   \n",
       "feature     tf_Ġtransportation  ts_spache_readability    tf_Ġif  tf_Ġrequired   \n",
       "\n",
       "                    153        154            155         156       157  \\\n",
       "importance     9.382985   9.347935       9.164958    9.125072  8.971872   \n",
       "feature     tf_Ġstudent  tf_Ġcould  ch_upper_frac  tf_Ġimpact  tf_Ġname   \n",
       "\n",
       "                  158              159           160       161  \\\n",
       "importance   8.706679         8.687664       8.57763    8.3493   \n",
       "feature     tf_Ġafter  va_arousal_mean  tf_Ġbenefits  tf_Ġboth   \n",
       "\n",
       "                          162       163         164       165          166  \\\n",
       "importance           8.201652  7.949665    7.875024   7.80212     7.797144   \n",
       "feature     ts_words_per_sent  tf_Ġwhat  tf_Ġshould  tf_Ġmuch  tf_Ġreasons   \n",
       "\n",
       "                  167           168                169       170       171  \\\n",
       "importance   7.544501      7.445221           7.244454  7.150042  6.944552   \n",
       "feature     tf_Ġmeans  tf_Ġlimiting  ts_syllable_count   tf_Ġnot   tf_Ġour   \n",
       "\n",
       "                              172       173                     174       175  \\\n",
       "importance               6.941762  6.930393                6.874111  6.838174   \n",
       "feature     ts_syllables_per_sent  tf_Ġyour  ts_flesch_reading_ease    tf_Ġam   \n",
       "\n",
       "                  176       177       178       179       180       181  \\\n",
       "importance   6.832476  6.773543  6.651915  6.580213  6.485192  6.468338   \n",
       "feature     tf_Ġpoint    tf_Ġit  tf_Ġthey  tf_Ġknow    tf_Ġat   tf_Ġget   \n",
       "\n",
       "                          182        183       184               185  \\\n",
       "importance           6.456057   6.327619  6.102661          6.045862   \n",
       "feature     va_dominance_mean  tf_Ġfocus   tf_Ġown  ws_sent_len_mean   \n",
       "\n",
       "                       186       187       188       189       190       191  \\\n",
       "importance        6.010983  5.981054  5.956406  5.908396  5.868488  5.848146   \n",
       "feature     tf_Ġeverything  tf_Ġkids    tf_Ġwe  tf_Ġhard  tf_Ġover  tf_Ġhome   \n",
       "\n",
       "                  192       193       194           195        196       197  \\\n",
       "importance   5.813245  5.774518  5.641491      5.617348   5.562448  5.496105   \n",
       "feature     tf_Ġthink   tf_Ġmay   tf_Ġcan  tf_Ġstudying  tf_Ġthing   tf_Ġtwo   \n",
       "\n",
       "                  198          199       200       201        202        203  \\\n",
       "importance   5.446288     4.977261  4.940655  4.803391   4.802198   4.739406   \n",
       "feature     tf_Ġoften  tf_Ġexample  tf_Ġsaid   tf_Ġall  tf_Ġwhile  tf_Ġfirst   \n",
       "\n",
       "                          204         205          206       207       208  \\\n",
       "importance           4.701181    4.675903     4.542944  4.366622  4.336646   \n",
       "feature     ts_mcalpine_eflaw  tf_Ġsports  tf_Ġhelping   tf_Ġwas  tf_Ġdone   \n",
       "\n",
       "                 209           210          211       212       213  \\\n",
       "importance  4.335607      4.310486      4.16504  4.124278  4.074976   \n",
       "feature     tf_Ġlast  tf_Ġfeedback  tf_Ġbelieve  tf_Ġjust    tf_Ġan   \n",
       "\n",
       "                   214          215       216       217         218  \\\n",
       "importance    4.058128     4.047112  4.033809  3.958862    3.883852   \n",
       "feature     tf_Ġpeople  tf_Ġanother   tf_Ġwhy   tf_Ġwho  tf_Ġreason   \n",
       "\n",
       "                     219          220       221       222       223       224  \\\n",
       "importance      3.794348     3.684112  3.624991  3.612345  3.540681  3.475764   \n",
       "feature     tf_Ġlearning  tf_Ġschools   tf_Ġone    tf_Ġno  tf_Ġkeep   tf_Ġbut   \n",
       "\n",
       "                  225        226       227       228       229       230  \\\n",
       "importance   3.435429   3.407918  3.379809  3.342614  3.306682  3.263934   \n",
       "feature     tf_Ġgoing  tf_Ġabout  tf_Ġwere  tf_Ġthan  tf_Ġstay   tf_Ġput   \n",
       "\n",
       "                 231             232       233       234         235  \\\n",
       "importance  3.259813        3.166806  3.154149  3.078102    2.938603   \n",
       "feature     tf_Ġgood  va_valence_std    tf_Ġbe  tf_Ġmake  tf_Ġalways   \n",
       "\n",
       "                 236       237       238       239       240       241  \\\n",
       "importance  2.877229  2.867808  2.849793  2.829164  2.787444   2.77756   \n",
       "feature      tf_Ġnow   tf_Ġhow   tf_Ġout    tf_Ġis  tf_Ġthis  tf_Ġwith   \n",
       "\n",
       "                 242       243        244                       245  \\\n",
       "importance  2.739072  2.726932   2.602082                  2.598131   \n",
       "feature       tf_Ġto   tf_Ġits  tf_Ġbeing  ts_linsear_write_formula   \n",
       "\n",
       "                   246       247       248         249        250       251  \\\n",
       "importance    2.557087  2.551506  2.533246    2.490699   2.484266  2.471044   \n",
       "feature     tf_Ġcreate   tf_Ġsay  tf_Ġsure  tf_Ġperson  tf_Ġusing  tf_Ġdown   \n",
       "\n",
       "                  252                  253      254       255       256  \\\n",
       "importance   2.448205             2.415814  2.31446  2.101922  2.099108   \n",
       "feature     tf_Ġwhich  ch_repeat_char_frac  tf_Ġfor  tf_Ġsame  tf_Ġtake   \n",
       "\n",
       "                 257       258      259       260        261        262  \\\n",
       "importance  2.086792  2.078264  1.99505  1.982076   1.962839   1.936546   \n",
       "feature      tf_Ġare  tf_Ġmore   tf_Ġin  tf_Ġwhen  tf_Ġthere  tf_Ġcause   \n",
       "\n",
       "                   263       264          265         266        267  \\\n",
       "importance    1.923023  1.905253     1.903507    1.870733    1.86094   \n",
       "feature     tf_Ġothers   tf_Ġfun  tf_Ġbenefit  tf_Ġthings  tf_Ġevery   \n",
       "\n",
       "                  268               269        270       271        272  \\\n",
       "importance   1.827483          1.821375   1.767868  1.741594   1.710009   \n",
       "feature     tf_Ġdoing  va_dominance_std  tf_Ġtheir  tf_Ġlife  tf_Ġlives   \n",
       "\n",
       "                 273         274       275       276       277       278  \\\n",
       "importance  1.684256    1.683862  1.640992  1.616394  1.573892  1.500577   \n",
       "feature        tf_Ġs  tf_Ġduring    tf_Ġup  tf_Ġhave  tf_Ġpart     tf_Ġa   \n",
       "\n",
       "                 279       280       281       282             283  \\\n",
       "importance  1.498889  1.496079   1.44943  1.437862        1.391529   \n",
       "feature     tf_Ġtime  tf_Ġonly  tf_Ġalso  tf_Ġthat  va_arousal_std   \n",
       "\n",
       "                      284         285       286       287       288  \\\n",
       "importance       1.379298    1.369891  1.338372  1.248961  1.240137   \n",
       "feature     tf_Ġdifferent  tf_Ġaround    tf_Ġor  tf_Ġthem    tf_Ġon   \n",
       "\n",
       "                    289       290       291      292       293      294  \\\n",
       "importance     1.197424  1.115942  1.110137  1.07143  1.045055  1.04131   \n",
       "feature     tf_Ġinstead  tf_Ġsome    tf_Ġas  tf_Ġnew    tf_Ġby   tf_Ġof   \n",
       "\n",
       "                 295        296       297       298         299         300  \\\n",
       "importance   1.03263   1.030561  1.013166  0.980068    0.973082    0.916443   \n",
       "feature     tf_Ġwant  tf_Ġthese  tf_Ġeven   tf_Ġway  tf_Ġbetter  tf_Ġnumber   \n",
       "\n",
       "                 301        302       303       304       305       306  \\\n",
       "importance  0.817195   0.816088  0.770794  0.759886  0.702525  0.693356   \n",
       "feature     tf_Ġable  tf_Ġother   tf_Ġsee  tf_Ġwork  tf_Ġidea  tf_Ġfrom   \n",
       "\n",
       "                 307       308       309       310       311         312  \\\n",
       "importance  0.686387  0.643064  0.625346  0.590192  0.586884    0.576589   \n",
       "feature     tf_Ġsuch   tf_Ġany   tf_Ġlot  tf_Ġfeel  tf_Ġlook  tf_Ġfuture   \n",
       "\n",
       "                 313       314          315        316        317  \n",
       "importance  0.562961  0.556713     0.546316   0.301447   0.283508  \n",
       "feature     tf_Ġhelp  tf_Ġwell  tf_Ġsomeone  tf_Ġgreat  tf_Ġtakes  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "scores = model.get_score(importance_type=\"gain\")\n",
    "assert len(scores)!=0\n",
    "rows = []\n",
    "for feature, score in scores.items():\n",
    "    rows.append({'importance': score, 'feature': feature})\n",
    "idf = pd.DataFrame.from_records(rows)\n",
    "idf = idf.sort_values([\"importance\"], ascending=False, ignore_index=True)\n",
    "fp = f\"{job_dir}/importance.csv\"\n",
    "idf.to_csv(fp, index=True)\n",
    "print(f\"Saved {fp}\")\n",
    "idf.T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a994ecf2-2058-4117-923f-bda3a05f83f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken 0:17:29.462461\n"
     ]
    }
   ],
   "source": [
    "tim.stop()\n",
    "print(f\"Total time taken {str(tim.elapsed)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
