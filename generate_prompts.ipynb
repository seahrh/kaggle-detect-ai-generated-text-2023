{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c38cb70-bf7d-4afd-ad47-a7f7122257ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pathlib\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel, LlamaForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, NamedTuple, Callable, Any\n",
    "import scml\n",
    "from scml import pandasx as pdx\n",
    "tim = scml.Timer()\n",
    "tim.start()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "percentiles=[.01, .05, .1, .2, .3, .4, .5, .6, .7, .8, .9, .95, .99]\n",
    "pd.set_option(\"use_inf_as_na\", True)\n",
    "pd.set_option(\"max_info_columns\", 9999)\n",
    "pd.set_option(\"display.max_columns\", 9999)\n",
    "pd.set_option(\"display.max_rows\", 9999)\n",
    "pd.set_option('max_colwidth', 9999)\n",
    "tqdm.pandas()\n",
    "scml.seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed1d756-0574-4d50-9091-3505034fbc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cpu')\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15abfa57-a0e6-4791-a95f-128460c4aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='huggingface/meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "pad_token_id=32000\n",
      "model_input_names=['input_ids', 'attention_mask']\n",
      "CPU times: user 29.4 ms, sys: 4.96 ms, total: 34.3 ms\n",
      "Wall time: 35.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface/meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "print(f\"{repr(tokenizer)}\\npad_token_id={tokenizer.pad_token_id}\\nmodel_input_names={tokenizer.model_input_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6febed68-985f-4722-ae3a-25b555b22aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f30cd1b6d034301b451c88eb6b9e18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32001, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
      ")\n",
      "CPU times: user 1.3 s, sys: 4.71 s, total: 6 s\n",
      "Wall time: 7.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Inference uses fp16\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16)\n",
    "model = model.to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4634e041-bd6f-40c5-86bc-2c2f125cd053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey, are you conscious? Can you talk to me?\\nI’m conscious, but I can’t talk.\\nI’m conscious, but I can’t talk. I’m conscious, but I can’t talk.\\nI’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk. I’m conscious, but I can’t talk.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=256)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f24a95-5ff7-45c3-acd1-d24036820944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken 0:00:35.554114\n"
     ]
    }
   ],
   "source": [
    "tim.stop()\n",
    "print(f\"Total time taken {str(tim.elapsed)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
